---
title: "svm"
author: "Daniel Hill"
format: pdf
editor: visual
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  warning = FALSE,
  message = FALSE,
  echo = TRUE,
  fig.width = 6,      
  fig.height = 4,     
  out.width = "80%",  
  fig.align = "center"
)
```

# Support Vector Machine Classification

In this document, I will implement support vector machine as the second supervised learning technique for classification. SVM has options for different kernals that we can tune, and I think this could be a good way to approach solving the challenges of seperating class A from class B that we saw when implementing logistic regression. It might be the case that A and D aren't linearly seperable, but a non linear kernal will be able to do it

## Setting up


```{r}
library(tidyverse)
library(e1071)      
library(caret)      
library(pROC)       


set.seed(54321)


data <- read_csv("data_clean.csv")
data$label <- as.factor(data$label)  

head(data)
```


```{r}

# create test and train data splits we can use for the rest of the modelling
training.samples <- data$label %>% 
  createDataPartition(p = 0.8, list = FALSE) # create 80:20 split for train:test
train_data  <- data[training.samples, ]
test_data <- data[-training.samples, ]


# check class distribution are similar in both splits
prop.table(table(train_data$label))
prop.table(table(test_data$label))

```

## Initial SVM model with linear kernel

Lets start with a basic SVM model using a linear kernel

```{r}

svm_linear <- svm(
  label ~ .,
  data = train_data,
  kernel = "linear",
  cost = 1, # will tune later - low value has softer margin on decision boundary. increasing will reduce misclassification, but increase risk of overfitting
  scale = FALSE       # data is already scaled during preprocessing
)


summary(svm_linear)

```
the support vectors are drawn from the edges of the 'envelope' that enclose each class to the decision boundary. So each one represents a point where the boundary is being defined. We can look at the total number of support vectors relative to our total number of data points, which is just under 3000. so 804/0.8*3000 = about a third of our data is being used to draw the decision boundary. 

we can also notice the distribution of support vectors between classes - we know that the distribution of class labels is approximately uniform, but there are significantly more support vectors for class D than the others.

```{r}
linear_predictions <- predict(svm_linear, test_data)


conf_matrix <- confusionMatrix(linear_predictions, test_data$label)
conf_matrix
```


```{r}

class_metrics <- conf_matrix$byClass

for (class in 1:nrow(class_metrics)) {
  cat("\nClass:", rownames(class_metrics)[class], "\n")
  cat("Sensitivity (Recall):", round(class_metrics[class, "Sensitivity"], 4), "\n")
  cat("Specificity:", round(class_metrics[class, "Specificity"], 4), "\n")
  cat("Precision (PPV):", round(class_metrics[class, "Pos Pred Value"], 4), "\n")
  cat("F1 Score:", round(class_metrics[class, "F1"], 4), "\n")
}
```

these results are slightly below the ones we got with logistic regression. Based on what we know about how both algorithms work, we know it should be possible to get svm to perform at least as well a logistic regression, so some tuning is in order,

The first thing will do is try some different kernals and see how they perform

## Exploring different kernels



```{r}

svm_poly_3 <- svm(
  label ~ .,
  data = train_data,
  kernel = "polynomial",
  degree = 3,         # polynomial degree, lets just try a few
  cost = 1,
  scale = FALSE
)

svm_poly_5 <- svm(
  label ~ .,
  data = train_data,
  kernel = "polynomial",
  degree = 5,         # polynomial degree, lets just try a few
  cost = 1,
  scale = FALSE
)

svm_poly_7 <- svm(
  label ~ .,
  data = train_data,
  kernel = "polynomial",
  degree = 7,         # polynomial degree, lets just try a few
  cost = 1,
  scale = FALSE
)


svm_radial <- svm(
  label ~ .,
  data = train_data,
  kernel = "radial",
  gamma = 0.05,       # tuneable parameter. idk what it does though
  cost = 1,
  scale = FALSE
)



svm_sigmoid <- svm(
  label ~ .,
  data = train_data,
  kernel = "sigmoid",
  gamma = 0.05,     # controls how steep/flat the sigmoid curve is. lower value is flatter and gives softer decision boundary
  coef0 = 0.5,    # Controls the y intercept shift of the sigmoid function - if 0 then the sigmoid goes throught he origin
  cost = 1,
  scale = FALSE   
)


for (model in list(svm_poly_3, svm_poly_5, svm_poly_7, svm_radial, svm_sigmoid)) {
  
  cat("\n\n\n\n\n")
  cat("\n", "summary:")
  print(summary(model))
  prediction <- predict(model, test_data)
  conf_matrix <- confusionMatrix(prediction, test_data$label)
  cat("\n", "confusion matrix")
  print(conf_matrix)
  class_metrics <- conf_matrix$byClass

  for (class in 1:nrow(class_metrics)) {
    cat("\nClass:", rownames(class_metrics)[class], "\n")
    cat("Sensitivity (Recall):", round(class_metrics[class, "Sensitivity"], 4), "\n")
    cat("Specificity:", round(class_metrics[class, "Specificity"], 4), "\n")
    cat("Precision (PPV):", round(class_metrics[class, "Pos Pred Value"], 4), "\n")
    cat("F1 Score:", round(class_metrics[class, "F1"], 4), "\n")
  }
}


```

the polynomial kernels start overfitting pretty quickly as we increase the order without ever producing an improved result. The sigmoid kernel was rubbish. The radial kernel performed slightly better that the linear one. This makes sense - the non-linear kernel is what would allow svm to seperate classes better than logistic regression, which is purely linear. Radial kernel is a very popular option.

lets move forward with the radial kernel and see if we can tune it for better results.


## Hyperparameter tuning

Now, let's tune the hyperparameters for the radial kernel to optimize the model:

```{r}


tune_grid <- expand.grid(
  C = c(0.01, 0.1, 1, 10, 100), # values of the cost parameter, lower values have softer margin
  sigma = c(0.01, 0.05, 0.1, 0.5, 1, 5) # values of the gamma parameter
)


svm_tune <- train(
  label ~ .,
  data = train_data,
  method = "svmRadial",
  preProcess = NULL, 
  tuneGrid = tune_grid,
  trControl = trainControl(method = "cv", number = 5) # cv is crossvalidation, 5 is the number of folds 
)
  1

print(svm_tune)
plot(svm_tune)
```
