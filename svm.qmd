---
title: "svm"
author: "Daniel Hill"
format: pdf
editor: visual
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  warning = FALSE,
  message = FALSE,
  echo = TRUE,
  fig.width = 6,      
  fig.height = 4,     
  out.width = "80%",  
  fig.align = "center"
)
```

# Support Vector Machine Classification

In this document, I will implement support vector machine as the second supervised learning technique for classification. SVM has options for different kernals that we can tune, and I think this could be a good way to approach solving the challenges of seperating class A from class B that we saw when implementing logistic regression. It might be the case that A and D aren't linearly seperable, but a non linear kernal will be able to do it

## Setting up

```{r}
library(tidyverse)
library(e1071)      
library(caret)      
library(pROC)       


set.seed(54321)


data <- read_csv("data_clean.csv")
data$label <- as.factor(data$label)  

head(data)
```

```{r}

# create test and train data splits we can use for the rest of the modelling
training.samples <- data$label %>% 
  createDataPartition(p = 0.8, list = FALSE) # create 80:20 split for train:test
train_data  <- data[training.samples, ]
test_data <- data[-training.samples, ]


# check class distribution are similar in both splits
prop.table(table(train_data$label))
prop.table(table(test_data$label))

```

## Initial SVM model with linear kernel

Lets start with a basic SVM model using a linear kernel

```{r}

svm_linear <- svm(
  label ~ .,
  data = train_data,
  kernel = "linear",
  cost = 1, # will tune later - low value has softer margin on decision boundary. increasing will reduce misclassification, but increase risk of overfitting
  scale = FALSE       # data is already scaled during preprocessing
)


summary(svm_linear)

```

the support vectors are drawn from the edges of the 'envelope' that enclose each class to the decision boundary. So each one represents a point where the boundary is being defined. We can look at the total number of support vectors relative to our total number of data points, which is just under 3000. so 804/0.8\*3000 = about a third of our data is being used to draw the decision boundary.

we can also notice the distribution of support vectors between classes - we know that the distribution of class labels is approximately uniform, but there are significantly more support vectors for class D than the others.

```{r}
linear_predictions <- predict(svm_linear, test_data)


conf_matrix <- confusionMatrix(linear_predictions, test_data$label)
conf_matrix
```

```{r}

class_metrics <- conf_matrix$byClass

for (class in 1:nrow(class_metrics)) {
  cat("\nClass:", rownames(class_metrics)[class], "\n")
  cat("Sensitivity (Recall):", round(class_metrics[class, "Sensitivity"], 4), "\n")
  cat("Specificity:", round(class_metrics[class, "Specificity"], 4), "\n")
  cat("Precision (PPV):", round(class_metrics[class, "Pos Pred Value"], 4), "\n")
  cat("F1 Score:", round(class_metrics[class, "F1"], 4), "\n")
}
```

these results are slightly below the ones we got with logistic regression. Based on what we know about how both algorithms work, we know it should be possible to get svm to perform at least as well a logistic regression, so some tuning is in order,

The first thing will do is try some different kernals and see how they perform

## Exploring different kernels

```{r}

svm_poly_3 <- svm(
  label ~ .,
  data = train_data,
  kernel = "polynomial",
  degree = 3,         # polynomial degree, lets just try a few
  cost = 1,
  scale = FALSE
)

svm_poly_5 <- svm(
  label ~ .,
  data = train_data,
  kernel = "polynomial",
  degree = 5,         # polynomial degree, lets just try a few
  cost = 1,
  scale = FALSE
)

svm_poly_7 <- svm(
  label ~ .,
  data = train_data,
  kernel = "polynomial",
  degree = 7,         # polynomial degree, lets just try a few
  cost = 1,
  scale = FALSE
)


svm_radial <- svm(
  label ~ .,
  data = train_data,
  kernel = "radial",
  gamma = 0.05,       # tuneable parameter. idk what it does though
  cost = 1,
  scale = FALSE
)



svm_sigmoid <- svm(
  label ~ .,
  data = train_data,
  kernel = "sigmoid",
  gamma = 0.05,     # controls how steep/flat the sigmoid curve is. lower value is flatter and gives softer decision boundary
  coef0 = 0.5,    # Controls the y intercept shift of the sigmoid function - if 0 then the sigmoid goes throught he origin
  cost = 1,
  scale = FALSE   
)


for (model in list(svm_poly_3, svm_poly_5, svm_poly_7, svm_radial, svm_sigmoid)) {
  
  cat("\n\n\n\n\n")
  cat("\n", "summary:")
  print(summary(model))
  prediction <- predict(model, test_data)
  conf_matrix <- confusionMatrix(prediction, test_data$label)
  cat("\n", "confusion matrix")
  print(conf_matrix)
  class_metrics <- conf_matrix$byClass

  for (class in 1:nrow(class_metrics)) {
    cat("\nClass:", rownames(class_metrics)[class], "\n")
    cat("Sensitivity (Recall):", round(class_metrics[class, "Sensitivity"], 4), "\n")
    cat("Specificity:", round(class_metrics[class, "Specificity"], 4), "\n")
    cat("Precision (PPV):", round(class_metrics[class, "Pos Pred Value"], 4), "\n")
    cat("F1 Score:", round(class_metrics[class, "F1"], 4), "\n")
  }
}


```

the polynomial kernels start overfitting pretty quickly as we increase the order without ever producing an improved result. The sigmoid kernel was rubbish. The radial kernel performed slightly better that the linear one. This makes sense - the non-linear kernel is what would allow svm to seperate classes better than logistic regression, which is purely linear. Radial kernel is a very popular option.

lets move forward with the radial kernel and see if we can tune it for better results.

## Hyperparameter tuning

Now, let's tune the hyperparameters for the radial kernel to optimize the model:

```{r}


tune_grid <- expand.grid(
  C = c(0.01, 0.1, 1, 10, 100), # values of the cost parameter, lower values have softer margin
  sigma = c(0.01, 0.05, 0.1, 0.5, 1) # values of the gamma parameter
)


svm_tune <- train(
  label ~ .,
  data = train_data,
  method = "svmRadial",
  preProcess = NULL, 
  tuneGrid = tune_grid,
  trControl = trainControl(method = "cv", number = 5) # cv is crossvalidation, 5 is the number of folds 
)
  1

print(svm_tune)
plot(svm_tune)
```

we can see that the model starts performing poorly quickly as the values get larger. So we will try again with smaller steps between 0 and 0.5

```{r}


tune_grid <- expand.grid(
  C = c(0.001, 0.01, 0.1, 0.2, 0.5), # values of the cost parameter, lower values have softer margin
  sigma = c(0.001, 0.01, 0.1, 0.2, 0.5) # values of the gamma parameter
)


svm_tune <- train(
  label ~ .,
  data = train_data,
  method = "svmRadial",
  preProcess = NULL, 
  tuneGrid = tune_grid,
  trControl = trainControl(method = "cv", number = 5) # cv is crossvalidation, 5 is the number of folds 
)
  1

print(svm_tune)
plot(svm_tune)
```

```{r}


tune_grid <- expand.grid(
  C = c(0.0025, 0.005, 0.0075, 0.01, 0.015, 0.02, 0.05, 0.075), # values of the cost parameter, lower values have softer margin
  sigma = c(0.0025, 0.005, 0.0075, 0.1, 0.015, 0.02, 0.05, 0.075) # values of the gamma parameter
)


svm_tune <- train(
  label ~ .,
  data = train_data,
  method = "svmRadial",
  preProcess = NULL, 
  tuneGrid = tune_grid,
  trControl = trainControl(method = "cv", number = 5) # cv is crossvalidation, 5 is the number of folds 
)
  1

print(svm_tune)
plot(svm_tune)
```

```{r}


tune_grid <- expand.grid(
  C = c(0.015, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07), # values of the cost parameter, lower values have softer margin
  sigma = c(0.015, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07) # values of the gamma parameter
)


svm_tune <- train(
  label ~ .,
  data = train_data,
  method = "svmRadial",
  preProcess = NULL, 
  tuneGrid = tune_grid,
  trControl = trainControl(method = "cv", number = 5) # cv is crossvalidation, 5 is the number of folds 
)
  1

print(svm_tune)
plot(svm_tune)
```

```{r}


tune_grid <- expand.grid(
  C = c(0.04, 0.05, 0.06, 0.07, 0.08), # values of the cost parameter, lower values have softer margin
  sigma = c(0.025, 0.03, 0.035, 0.04, 0.045, 0.05, 0.055) # values of the gamma parameter
)


svm_tune <- train(
  label ~ .,
  data = train_data,
  method = "svmRadial",
  preProcess = NULL, 
  tuneGrid = tune_grid,
  trControl = trainControl(method = "cv", number = 5) # cv is crossvalidation, 5 is the number of folds 
)
  1

print(svm_tune)
plot(svm_tune)
```

```{r}


tune_grid <- expand.grid(
  C = c(0.06, 0.07, 0.08, 0.09, 0.1), # values of the cost parameter, lower values have softer margin
  sigma = c(0.037, 0.038, 0.039, 0.04, 0.041, 0.042, 0.043, 0.044, 0.045) # values of the gamma parameter
)


svm_tune <- train(
  label ~ .,
  data = train_data,
  method = "svmRadial",
  preProcess = NULL, 
  tuneGrid = tune_grid,
  trControl = trainControl(method = "cv", number = 5) # cv is crossvalidation, 5 is the number of folds 
)
  1

print(svm_tune)
plot(svm_tune)
```

```{r}


tune_grid <- expand.grid(
  C = c(0.1, 0.11, 0.12), # values of the cost parameter, lower values have softer margin
  sigma = c(0.04, 0.041, 0.042, 0.043, 0.044, 0.045, 0.046, 0.047, 0.048, 0.049, 0.05) # values of the gamma parameter
)


svm_tune <- train(
  label ~ .,
  data = train_data,
  method = "svmRadial",
  preProcess = NULL, 
  tuneGrid = tune_grid,
  trControl = trainControl(method = "cv", number = 5) # cv is crossvalidation, 5 is the number of folds 
)
  1

print(svm_tune)
plot(svm_tune)
```

```{r}


tune_grid <- expand.grid(
  C = c(0.1, 0.11, 0.12, 0.13, 0.14), # values of the cost parameter, lower values have softer margin
  sigma = c(0.039, 0.04, 0.041, 0.042, 0.043, 0.044, 0.045, 0.046) # values of the gamma parameter
)


svm_tune <- train(
  label ~ .,
  data = train_data,
  method = "svmRadial",
  preProcess = NULL, 
  tuneGrid = tune_grid,
  trControl = trainControl(method = "cv", number = 5) # cv is crossvalidation, 5 is the number of folds 
)
  1

print(svm_tune)
plot(svm_tune)
```

```{r}


tune_grid <- expand.grid(
  C = c(0.1, 0.11, 0.12, 0.13, 0.14, 0.15), # values of the cost parameter, lower values have softer margin
  sigma = c(0.041, 0.042, 0.043, 0.044, 0.045, 0.046, 0.047) # values of the gamma parameter
)


svm_tune <- train(
  label ~ .,
  data = train_data,
  method = "svmRadial",
  preProcess = NULL, 
  tuneGrid = tune_grid,
  trControl = trainControl(method = "cv", number = 5) # cv is crossvalidation, 5 is the number of folds 
)
  1

print(svm_tune)
plot(svm_tune)
```

```{r}


tune_grid <- expand.grid(
  C = c(0.15, 0.155, 0.16, 0.165, 0.17, 0.175, 0.18), # values of the cost parameter, lower values have softer margin
  sigma = c(0.043, 0.044, 0.045, 0.046, 0.047, 0.048, 0.049) # values of the gamma parameter
)


svm_tune <- train(
  label ~ .,
  data = train_data,
  method = "svmRadial",
  preProcess = NULL, 
  tuneGrid = tune_grid,
  trControl = trainControl(method = "cv", number = 5) # cv is crossvalidation, 5 is the number of folds 
)
  1

print(svm_tune)
plot(svm_tune)
```

```{r}


tune_grid <- expand.grid(
  C = c(0.165, 0.17, 0.175, 0.18), # values of the cost parameter, lower values have softer margin
  sigma = c(0.045, 0.046, 0.047, 0.048, 0.049, 0.05, 0.051, 0.052, 0.053) # values of the gamma parameter
)


svm_tune <- train(
  label ~ .,
  data = train_data,
  method = "svmRadial",
  preProcess = NULL, 
  tuneGrid = tune_grid,
  trControl = trainControl(method = "cv", number = 5) # cv is crossvalidation, 5 is the number of folds 
)
  1

print(svm_tune)
plot(svm_tune)
```

```{r}


tune_grid <- expand.grid(
  C = c(0.17, 0.175, 0.18, 0.19), # values of the cost parameter, lower values have softer margin
  sigma = c(0.044, 0.0445, 0.045, 0.0455, 0.046, 0.0465, 0.047, 0.048, 0.049, 0.05) # values of the gamma parameter
)


svm_tune <- train(
  label ~ .,
  data = train_data,
  method = "svmRadial",
  preProcess = NULL, 
  tuneGrid = tune_grid,
  trControl = trainControl(method = "cv", number = 5) # cv is crossvalidation, 5 is the number of folds 
)
  1

print(svm_tune)
plot(svm_tune)
```

```{r}


tune_grid <- expand.grid(
  C = c(0.172, 0.174, 0.176, 0.178, 0.18, 0.182, 0.185, 0.187, 0.19, 0.192), # values of the cost parameter, lower values have softer margin
  sigma = c(0.04, 0.041, 0.044, 0.0445, 0.045, 0.0455, 0.046, 0.0465, 0.047, 0.048, 0.049, 0.05, 0.051, 0.052) # values of the gamma parameter
)


svm_tune <- train(
  label ~ .,
  data = train_data,
  method = "svmRadial",
  preProcess = NULL, 
  tuneGrid = tune_grid,
  trControl = trainControl(method = "cv", number = 5) # cv is crossvalidation, 5 is the number of folds 
)
  1

print(svm_tune)
plot(svm_tune)
```

At this point, I'm seeing that the variation due to cross validation instability is greater than the increases in accuracy and agreement that we get from continuing to refine the hyperparameters.

I am going to do another run with a higher number of folds and some repeats to try and get a more stable value. This is valuable because we want to find a parameter that performs well on repeated runs not just one specific run.

```{r}


tune_grid <- expand.grid(
  C = c(0.17, 0.172, 0.174, 0.176, 0.178, 0.18, 0.182, 0.185, 0.187, 0.19, 0.192, 0.195), # values of the cost parameter, lower values have softer margin
  sigma = c(0.04, 0.041, 0.044, 0.0445, 0.045, 0.0455, 0.046, 0.0465, 0.047, 0.048, 0.049, 0.05, 0.051, 0.052, 0.053) # values of the gamma parameter
)


svm_tune <- train(
  label ~ .,
  data = train_data,
  method = "svmRadial",
  preProcess = NULL, 
  tuneGrid = tune_grid,
  trControl = trainControl(method = "repeatedcv", number = 10, repeats = 3) # cv is crossvalidation, 5 is the number of folds 
  # with three repeats, the tuning is run 3 times and the average results are returned
)
  1

print(svm_tune)
plot(svm_tune)
```

```{r}
# this will take forever, im leaving it running overnight

tune_grid <- expand.grid(
  C = c(0.164, 0.166, 0.168, 0.17, 0.172, 0.174, 0.176, 0.178, 0.18, 0.182, 0.185, 0.187, 0.19, 0.192, 0.195, 0.1975, 0.2, 0.202), # values of the cost parameter, lower values have softer margin
  sigma = c(0.038, 0.039, 0.04, 0.041, 0.044, 0.0445, 0.045, 0.0455, 0.046, 0.0465, 0.047, 0.048, 0.049, 0.05, 0.051, 0.052, 0.053, 0.054, 0.055) # values of the gamma parameter
)


svm_tune <- train(
  label ~ .,
  data = train_data,
  method = "svmRadial",
  preProcess = NULL, 
  tuneGrid = tune_grid,
  trControl = trainControl(method = "repeatedcv", number = 10, repeats = 5) # cv is crossvalidation, 5 is the number of folds 
  # with three repeats, the tuning is run 3 times and the average results are returned
)
  1

print(svm_tune)
plot(svm_tune)
```

okay all of these are within 0.25% accuracy so i think we can take the highest value to use for training an optimized model

## Final model with optimal parameters

Now let's train our final model with the optimal hyperparameters:

```{r}
# Extract optimal parameters
optimal_params <- svm_tune$bestTune
optimal_params
```

```{r}



final_model <- svm(
  label ~ .,
  data = train_data,
  kernel = "radial",
  gamma = optimal_params$sigma,
  cost = optimal_params$C,
  scale = FALSE
)

final_predictions <- predict(final_model, test_data)


final_conf_matrix <- confusionMatrix(final_predictions, test_data$label)
final_conf_matrix

final_class_metrics <- final_conf_matrix$byClass
for (class in 1:nrow(final_class_metrics)) {
  cat("\nClass:", rownames(final_class_metrics)[class], "\n")
  cat("Sensitivity (Recall):", round(final_class_metrics[class, "Sensitivity"], 4), "\n")
  cat("Specificity:", round(final_class_metrics[class, "Specificity"], 4), "\n")
  cat("Precision (PPV):", round(final_class_metrics[class, "Pos Pred Value"], 4), "\n")
  cat("F1 Score:", round(final_class_metrics[class, "F1"], 4), "\n")
}
```

the final results aren't bad, better than logistic regression but not as good as the random forest. We still haven't found a model that is good at seperating class D. In production, we would probabaly try to find a one-vs-all approach for class D specifically, and use it in combination with a tuned random forest model.

## Feature selection for SVM

like i did with other models, im going to look into if we can improve the model by selecting only the most important features based on our previous findings:

```{r}

selected_train_data <- train_data %>%
  select(X3, X7, X8, X9, X10, X11, X12, X14, label)

selected_test_data <- test_data %>%
  select(X3, X7, X8, X9, X10, X11, X12, X14, label)


selected_model <- svm(
  label ~ .,
  data = selected_train_data,
  kernel = "radial",
  cost = optimal_params$C,
  gamma = optimal_params$sigma,
  scale = FALSE
)


selected_predictions <- predict(selected_model, selected_test_data)


selected_conf_matrix <- confusionMatrix(selected_predictions, selected_test_data$label)
selected_conf_matrix


selected_metrics <- selected_conf_matrix$byClass
for (class in 1:nrow(selected_metrics)) {
  cat("\nClass:", rownames(selected_metrics)[class], "\n")
  cat("Sensitivity (Recall):", round(selected_metrics[class, "Sensitivity"], 4), "\n")
  cat("Specificity:", round(selected_metrics[class, "Specificity"], 4), "\n")
  cat("Precision (PPV):", round(selected_metrics[class, "Pos Pred Value"], 4), "\n")
  cat("F1 Score:", round(selected_metrics[class, "F1"], 4), "\n")
}

```

worse :shrug:

## analysis for Class D

lets look at a one-vs-all model for class D to see if we can get better results for D than we did with the multinomial models

```{r}

train_data_binary <- train_data %>%
  mutate(label = factor( ifelse(label == "D", 1, 0), levels = c(0, 1) )) # create binary classifier for D vs not D

test_data_binary <- test_data %>%
  mutate(label = factor( ifelse(label == "D", 1, 0), levels = c(0, 1) )) # 

  

d_vs_rest_model <- svm(
  label ~ .,
  data = train_data_binary,
  kernel = "radial",
  cost = 1,
  scale = FALSE,
  probability = TRUE # need this for the roc analysis to compare to logistic regression roc
)

class_d_predictions <- predict(d_vs_rest_model, test_data_binary, probability = FALSE)
prob_predictions <- predict(d_vs_rest_model, test_data_binary, probability = TRUE)
class_d_probs <- attr(prob_predictions, "probabilities")[, 1]

one_class_conf_matrix <- confusionMatrix(class_d_predictions, test_data_binary$label)
one_class_conf_matrix


```

By making the dataset binary we have made it unbalanced - there are now three times as many negative samples as positive samples. This means that the accuracy metric is less meaningful - its going to be higher just because the model now has a very high true negative rate from correctly identifying instances of B and C that aren't class D and this is larger than the false negative rate from misclassifying class D instances. However, despite the high accuracy metric achieved by this model, we can see by looking at the positive value reference column of the confusion matrix and the low specificity score that the model still struggles to correctly classify class D.

```{r}

roc_obj <- roc(is_class_d, class_d_probs)
auc_value <- auc(roc_obj)


plot(roc_obj, main = paste("ROC Curve: Class D vs. All (AUC =", round(auc_value, 3), ")"),
     col = "blue", lwd = 2, 
     xlab = "False Positive Rate (1 - Specificity)", 
     ylab = "True Positive Rate (Sensitivity)")
abline(a = 0, b = 1, lty = 2, col = "gray")
```

```{r}

ggplot(test_data, aes(x = X7, y = X8, color = label, shape = label)) +
  geom_point(size = 3, alpha = 0.7) +
  labs(title = "Feature Space: X7 vs X8",
       x = "X7", y = "X8",
       color = "Actual Class",
       shape = "Actual Class") +
  theme_minimal()


prediction_df <- data.frame(
  X7 = test_data$X7,
  X8 = test_data$X8,
  Actual = test_data$label,
  Predicted = final_predictions,
  Correct = test_data$label == final_predictions
)

ggplot(prediction_df, aes(x = X7, y = X8, color = Predicted, shape = Actual)) +
  geom_point(size = 3, alpha = 0.7) +
  labs(title = "SVM Classification: X7 vs X8",
       x = "X7", y = "X8",
       color = "Predicted Class",
       shape = "Actual Class") +
  theme_minimal()


ggplot(prediction_df, aes(x = X7, y = X8, color = Correct, shape = Actual)) +
  geom_point(size = 3, alpha = 0.7) +
  scale_color_manual(values = c("TRUE" = "green", "FALSE" = "red")) +
  labs(title = "SVM Misclassifications: X7 vs X8",
       x = "X7", y = "X8",
       color = "Correctly Classified",
       shape = "Actual Class") +
  theme_minimal()
```

## Using bootstrap data

now we are going to apply our final svm model to the bootstrap data:

```{r}
data <- read_csv("data_clean.csv")
data$label <- as.factor(data$label)

# create test and train data splits we can use for the rest of the modelling
training.samples <- data$label %>% 
  createDataPartition(p = 0.8, list = FALSE) # create 70:30 split for train:test. choose 70:30 instead of 80:20 since we have lots of data and we are looking to increase the amount of test data for more granular results
train_data  <- data[training.samples, ]
test_data <- data[-training.samples, ]

bootstrap_train_data <- train_data %>% sample_n(size=10000, replace=TRUE)
bootstrap_test_data <- test_data %>% sample_n(size=10000, replace=TRUE)




bootstrap_model <- svm(
  label ~ .,
  data = bootstrap_train_data,
  kernel = "radial",
  cost = optimal_params$C,
  gamma = optimal_params$sigma,
  scale = FALSE
)


bootstrap_predictions <- predict(bootstrap_model, bootstrap_test_data)
bootstrap_conf_matrix <- confusionMatrix(bootstrap_predictions, bootstrap_test_data$label)
bootstrap_conf_matrix
```

```{r}

cat("Original data accuracy:", round(final_conf_matrix$overall["Accuracy"], 4), "\n")
cat("Bootstrap data accuracy:", round(bootstrap_conf_matrix$overall["Accuracy"], 4), "\n")



```
