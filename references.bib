@misc{a5c1d2h2i1m1n2o1r2t1AnswerConvertCategorical2017,
  title = {Answer to "{{Convert}} Categorical Variables to Numeric in {{R}}"},
  author = {A5C1D2H2I1M1N2O1R2T1},
  year = {2017},
  month = dec,
  journal = {Stack Overflow},
  urldate = {2025-03-23},
  file = {C:\Users\Daniel\Zotero\storage\QQIG3R3K\convert-categorical-variables-to-numeric-in-r.html}
}

@misc{BayesianInformationCriterion,
  title = {Bayesian {{Information Criterion}} - an Overview {\textbar} {{ScienceDirect Topics}}},
  urldate = {2025-04-26},
  howpublished = {https://www.sciencedirect.com/topics/medicine-and-dentistry/bayesian-information-criterion},
  file = {C:\Users\Daniel\Zotero\storage\E4PSWFM6\bayesian-information-criterion.html}
}

@article{BayesianInformationCriterion2025,
  title = {Bayesian Information Criterion},
  year = {2025},
  month = apr,
  journal = {Wikipedia},
  urldate = {2025-04-26},
  abstract = {In statistics, the Bayesian information criterion (BIC) or Schwarz information criterion (also SIC, SBC, SBIC) is a criterion for model selection among a finite set of models; models with lower BIC are generally preferred. It is based, in part, on the likelihood function and it is closely related to the Akaike information criterion (AIC). When fitting models, it is possible to increase the maximum likelihood by adding parameters, but doing so may result in overfitting. Both BIC and AIC attempt to resolve this problem by introducing a penalty term for the number of parameters in the model; the penalty term is larger in BIC than in AIC for sample sizes greater than 7. The BIC was developed by Gideon E. Schwarz and published in a 1978 paper, as a large-sample approximation to the Bayes factor.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1286138126},
  file = {C:\Users\Daniel\Zotero\storage\WEX2GAV8\Bayesian_information_criterion.html}
}

@misc{caAnswerConvertCategorical2018,
  title = {Answer to "{{Convert}} Categorical Variables to Numeric in {{R}}"},
  author = {Ca, Pat},
  year = {2018},
  month = may,
  journal = {Stack Overflow},
  urldate = {2025-03-23},
  file = {C:\Users\Daniel\Zotero\storage\WAMXARFD\convert-categorical-variables-to-numeric-in-r.html}
}

@article{caiESPRITForestParallelClustering2017,
  title = {{{ESPRIT-Forest}}: {{Parallel}} Clustering of Massive Amplicon Sequence Data in Subquadratic Time},
  shorttitle = {{{ESPRIT-Forest}}},
  author = {Cai, Yunpeng and Zheng, Wei and Yao, Jin and Yang, Yujie and Mai, Volker and Mao, Qi and Sun, Yijun},
  year = {2017},
  month = apr,
  journal = {PLOS Computational Biology},
  volume = {13},
  number = {4},
  pages = {e1005518},
  publisher = {Public Library of Science},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1005518},
  urldate = {2025-04-29},
  abstract = {The rapid development of sequencing technology has led to an explosive accumulation of genomic sequence data. Clustering is often the first step to perform in sequence analysis, and hierarchical clustering is one of the most commonly used approaches for this purpose. However, it is currently computationally expensive to perform hierarchical clustering of extremely large sequence datasets due to its quadratic time and space complexities. In this paper we developed a new algorithm called ESPRIT-Forest for parallel hierarchical clustering of sequences. The algorithm achieves subquadratic time and space complexity and maintains a high clustering accuracy comparable to the standard method. The basic idea is to organize sequences into a pseudo-metric based partitioning tree for sub-linear time searching of nearest neighbors, and then use a new multiple-pair merging criterion to construct clusters in parallel using multiple threads. The new algorithm was tested on the human microbiome project (HMP) dataset, currently one of the largest published microbial 16S rRNA sequence dataset. Our experiment demonstrated that with the power of parallel computing it is now compu- tationally feasible to perform hierarchical clustering analysis of tens of millions of sequences. The software is available at http://www.acsu.buffalo.edu/{$\sim$}yijunsun/lab/ESPRIT-Forest.html.},
  langid = {english},
  keywords = {Algorithms,Hierarchical clustering,Microbiome,Phylogenetic analysis,Ribosomal RNA,Sequence alignment,Taxonomy,Trees},
  file = {C:\Users\Daniel\Zotero\storage\F482JSPN\Cai et al. - 2017 - ESPRIT-Forest Parallel clustering of massive amplicon sequence data in subquadratic time.pdf}
}

@article{caiESPRITTreeHierarchicalClustering2011,
  title = {{{ESPRIT-Tree}}: Hierarchical Clustering Analysis of Millions of {{16S rRNA}} Pyrosequences in Quasilinear Computational Time},
  shorttitle = {{{ESPRIT-Tree}}},
  author = {Cai, Yunpeng and Sun, Yijun},
  year = {2011},
  month = aug,
  journal = {Nucleic Acids Research},
  volume = {39},
  number = {14},
  pages = {e95},
  issn = {0305-1048},
  doi = {10.1093/nar/gkr349},
  urldate = {2025-04-29},
  abstract = {Taxonomy-independent analysis plays an essential role in microbial community analysis. Hierarchical clustering is one of the most widely employed approaches to finding operational taxonomic units, the basis for many downstream analyses. Most existing algorithms have quadratic space and computational complexities, and thus can be used only for small or medium-scale problems. We propose a new online learning-based algorithm that simultaneously addresses the space and computational issues of prior work. The basic idea is to partition a sequence space into a set of subspaces using a partition tree constructed using a pseudometric, then recursively refine a clustering structure in these subspaces. The technique relies on new methods for fast closest-pair searching and efficient dynamic insertion and deletion of tree nodes. To avoid exhaustive computation of pairwise distances between clusters, we represent each cluster of sequences as a probabilistic sequence, and define a set of operations to align these probabilistic sequences and compute genetic distances between them. We present analyses of space and computational complexity, and demonstrate the effectiveness of our new algorithm using a human gut microbiota data set with over one million sequences. The new algorithm exhibits a quasilinear time and space complexity comparable to greedy heuristic clustering algorithms, while achieving a similar accuracy to the standard hierarchical clustering algorithm.},
  pmcid = {PMC3152367},
  pmid = {21596775},
  file = {C:\Users\Daniel\Zotero\storage\KQG2374Y\Cai and Sun - 2011 - ESPRIT-Tree hierarchical clustering analysis of millions of 16S rRNA pyrosequences in quasilinear c.pdf}
}

@inproceedings{chenXGBoostScalableTree2016,
  title = {{{XGBoost}}: {{A Scalable Tree Boosting System}}},
  shorttitle = {{{XGBoost}}},
  booktitle = {Proceedings of the 22nd {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Chen, Tianqi and Guestrin, Carlos},
  year = {2016},
  month = aug,
  eprint = {1603.02754},
  primaryclass = {cs},
  pages = {785--794},
  doi = {10.1145/2939672.2939785},
  urldate = {2025-04-29},
  abstract = {Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\Daniel\\Zotero\\storage\\AAWQ8AC7\\Chen and Guestrin - 2016 - XGBoost A Scalable Tree Boosting System.pdf;C\:\\Users\\Daniel\\Zotero\\storage\\F94FXZVW\\1603.html}
}

@misc{ConvertCategoricalVariables2017,
  type = {Forum Post},
  title = {Convert Categorical Variables to Numeric in {{R}}},
  year = {2017},
  month = dec,
  journal = {Stack Overflow},
  urldate = {2025-03-23},
  file = {C:\Users\Daniel\Zotero\storage\9D4HK5V6\convert-categorical-variables-to-numeric-in-r.html}
}

@misc{dmi3knoAnswerConvertCategorical2017,
  title = {Answer to "{{Convert}} Categorical Variables to Numeric in {{R}}"},
  author = {{dmi3kno}},
  year = {2017},
  month = dec,
  journal = {Stack Overflow},
  urldate = {2025-03-23},
  file = {C:\Users\Daniel\Zotero\storage\4356QNY3\convert-categorical-variables-to-numeric-in-r.html}
}

@misc{FunctionReference,
  title = {Function Reference},
  urldate = {2025-04-02},
  howpublished = {https://ggplot2.tidyverse.org/reference/index.html},
  langid = {english},
  file = {C:\Users\Daniel\Zotero\storage\8QNPGSPZ\index.html}
}

@misc{Functions,
  title = {R {{Functions}}},
  urldate = {2025-04-26},
  abstract = {W3Schools offers free online tutorials, references and exercises in all the major languages of the web. Covering popular subjects like HTML, CSS, JavaScript, Python, SQL, Java, and many, many more.},
  howpublished = {https://www.w3schools.com/r/r\_functions.asp},
  langid = {american},
  file = {C:\Users\Daniel\Zotero\storage\2C9Y7NC2\r_functions.html}
}

@misc{GaussianMixtureModel,
  title = {Gaussian {{Mixture Model Explained}}},
  journal = {Built In},
  urldate = {2025-04-26},
  abstract = {A Gaussian mixture model is a soft clustering machine learning method used to determine the probability each data point belongs to a given cluster. Learn more.},
  howpublished = {https://builtin.com/articles/gaussian-mixture-model},
  langid = {english},
  file = {C:\Users\Daniel\Zotero\storage\49IF92MW\gaussian-mixture-model.html}
}

@book{geronHandsonMachineLearning2023,
  title = {Hands-on Machine Learning with {{Scikit-Learn}}, {{Keras}} and {{TensorFlow}}: Concepts, Tools, and Techniques to Build Intelligent Systems},
  shorttitle = {Hands-on Machine Learning with {{Scikit-Learn}}, {{Keras}} and {{TensorFlow}}},
  author = {G{\'e}ron, Aur{\'e}lien},
  year = {2023},
  edition = {Third edition},
  publisher = {O'Reilly Media, Inc},
  address = {Beijing},
  abstract = {"Through a recent series of breakthroughs, deep learning has boosted the entire field of machine learning. Now, even programmers who know close to nothing about this technology can use simple, efficient tools to implement programs capable of learning from data. This best-selling book uses concrete examples, minimal theory, and production-ready Python frameworks--scikit-learn, Keras, and TensorFlow--to help you gain an intuitive understanding of the concepts and tools for building intelligent systems. With this updated third edition, author Aurelien Geron explores a range of techniques, starting with simple linear regression and progressing to deep neural networks. Numerous code examples and exercises throughout the book help you apply what you've learned. Programming experience is all you need to get started"--},
  isbn = {978-1-0981-2597-4},
  lccn = {QA76.73.P98 G45 2023},
  keywords = {Apprentissage automatique,artificial intelligence,Artificial intelligence,Intelligence artificielle,Machine learning,Python (Computer program language),Python (Langage de programmation),TensorFlow},
  annotation = {OCLC: on1346503549}
}

@book{grusDataScienceScratch2019,
  title = {Data {{Science}} from {{Scratch}}: {{First Principles}} with {{Python}}},
  shorttitle = {Data {{Science}} from {{Scratch}}},
  author = {Grus, Joel},
  year = {2019},
  edition = {2nd ed},
  publisher = {O'Reilly Media, Incorporated},
  address = {Sebastopol},
  abstract = {Cover -- Copyright -- Table of Contents -- Preface to the Second Edition -- Conventions Used in This Book -- Using Code Examples -- O'Reilly Online Learning -- How to Contact Us -- Acknowledgments -- Preface to the First Edition -- Data Science -- From Scratch -- Chapter 1. Introduction -- The Ascendance of Data -- What Is Data Science? -- Motivating Hypothetical: DataSciencester -- Finding Key Connectors -- Data Scientists You May Know -- Salaries and Experience -- Paid Accounts -- Topics of Interest -- Onward -- Chapter 2. A Crash Course in Python -- The Zen of Python -- Getting Python -- Virtual Environments -- Whitespace Formatting -- Modules -- Functions -- Strings -- Exceptions -- Lists -- Tuples -- Dictionaries -- defaultdict -- Counters -- Sets -- Control Flow -- Truthiness -- Sorting -- List Comprehensions -- Automated Testing and assert -- Object-Oriented Programming -- Iterables and Generators -- Randomness -- Regular Expressions -- Functional Programming -- zip and Argument Unpacking -- args and kwargs -- Type Annotations -- How to Write Type Annotations -- Welcome to DataSciencester! -- For Further Exploration -- Chapter 3. Visualizing Data -- matplotlib -- Bar Charts -- Line Charts -- Scatterplots -- For Further Exploration -- Chapter 4. Linear Algebra -- Vectors -- Matrices -- For Further Exploration -- Chapter 5. Statistics -- Describing a Single Set of Data -- Central Tendencies -- Dispersion -- Correlation -- Simpson's Paradox -- Some Other Correlational Caveats -- Correlation and Causation -- For Further Exploration -- Chapter 6. Probability -- Dependence and Independence -- Conditional Probability -- Bayes's Theorem -- Random Variables -- Continuous Distributions -- The Normal Distribution -- The Central Limit Theorem -- For Further Exploration -- Chapter 7. Hypothesis and Inference -- Statistical Hypothesis Testing},
  isbn = {978-1-4920-4113-9 978-1-4920-4110-8},
  langid = {english}
}

@misc{HastiesudomainsISLPISLP_websitepdfdownloadhtml,
  title = {Hastie.Su.Domains/{{ISLP}}/{{ISLP}}\_website.Pdf.Download.Html},
  urldate = {2025-04-29},
  howpublished = {https://hastie.su.domains/ISLP/ISLP\_website.pdf.download.html},
  file = {C:\Users\Daniel\Zotero\storage\WHLZEIR8\ISLP_website.pdf.download.html}
}

@misc{hillAdvancedstatistics2025,
  title = {Advanced-Statistics},
  author = {Hill, Daniel Martin},
  year = {2025},
  month = apr,
  abstract = {The code repository for working files and final reports   completed for my final assessment of Advanced Statistics -   a module of UWE's MSci Data Science programme.},
  annotation = {https://github.com/danielmh111/advanced-statistics\\
danielmartinhill@gmail.com}
}

@software{Hill_Advanced-Statistics,
author = {Hill, Daniel Martin},
title = {{Advanced-Statistics}},
url = {https://github.com/danielmh111/advanced-statistics}
}

@misc{hillAdvancedstatistics2025a,
  title = {Advanced-Statistics},
  author = {Hill, Daniel Martin},
  year = {2025},
  month = apr,
  abstract = {The code repository for working files and final reports   completed for my final assessment of Advanced Statistics -   a module of UWE's MSci Data Science programme.},
  annotation = {https://github.com/danielmh111/advanced-statistics\\
danielmartinhill@gmail.com}
}

@misc{HopkinsFunctionRDocumentation,
  title = {Hopkins Function - {{RDocumentation}}},
  urldate = {2025-04-02},
  howpublished = {https://www.rdocumentation.org/packages/clustertend/versions/1.4/topics/hopkins},
  file = {C:\Users\Daniel\Zotero\storage\CW9CKU47\hopkins.html}
}

@article{HopkinsStatistic2025,
  title = {Hopkins Statistic},
  year = {2025},
  month = jan,
  journal = {Wikipedia},
  urldate = {2025-04-02},
  abstract = {The Hopkins statistic (introduced by Brian Hopkins and John Gordon Skellam) is a way of measuring the cluster tendency of a data set. It belongs to the family of sparse sampling tests. It acts as a statistical hypothesis test where the null hypothesis is that the data is generated by a Poisson point process and are thus uniformly randomly distributed. If individuals are aggregated, then its value approaches 0, and if they are randomly distributed along the value tends to 0.5.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1268036136},
  file = {C:\Users\Daniel\Zotero\storage\UKCEXL7Z\Hopkins_statistic.html}
}

@misc{HowCreateCorrelation23:40:18+00:00,
  title = {How to {{Create Correlation Heatmap}} in {{R}}},
  year = {23:40:18+00:00},
  journal = {GeeksforGeeks},
  urldate = {2025-03-23},
  abstract = {A Computer Science portal for geeks. It contains well written, well thought and well explained computer science and programming articles, quizzes and practice/competitive programming/company interview Questions.},
  chapter = {R Language},
  howpublished = {https://www.geeksforgeeks.org/how-to-create-correlation-heatmap-in-r/},
  langid = {american},
  file = {C:\Users\Daniel\Zotero\storage\NGHWFDQT\how-to-create-correlation-heatmap-in-r.html}
}

@misc{HowExplainGradient,
  title = {How to Explain Gradient Boosting},
  urldate = {2025-04-29},
  abstract = {3-part article on how gradient boosting works for squared error, absolute error, and general loss functions. Deeply explained, but as simply and intuitively as possible.},
  howpublished = {http://explained.ai/gradient-boosting/index.html},
  file = {C:\Users\Daniel\Zotero\storage\H2CVJB9A\index.html}
}

@misc{IfElseConditions,
  title = {R {{If}}...{{Else Conditions}}},
  urldate = {2025-04-26},
  abstract = {W3Schools offers free online tutorials, references and exercises in all the major languages of the web. Covering popular subjects like HTML, CSS, JavaScript, Python, SQL, Java, and many, many more.},
  howpublished = {https://www.w3schools.com/r/r\_if\_else.asp},
  langid = {american},
  file = {C:\Users\Daniel\Zotero\storage\ALQQLAUY\r_if_else.html}
}

@misc{Initialisation,
  title = {Initialisation},
  urldate = {2025-04-26},
  abstract = {mclust is a contributed R package for model-based clustering, classification, and density estimation based on finite normal mixture modelling. It provides functions for parameter estimation via the EM algorithm for normal mixture models with a variety of covariance structures, and functions for simulation from these models. Also included are functions that combine model-based hierarchical clustering, EM for mixture estimation and the Bayesian Information Criterion (BIC) in comprehensive strategies for clustering, density estimation and discriminant analysis. Additional functionalities are available for displaying and visualizing fitted models along with clustering, classification, and density estimation results.}
}

@book{Introduction,
  title = {Introduction}
}

@book{jamesIntroductionStatisticalLearning2023,
  title = {An Introduction to Statistical Learning: With Applications in {{Python}}},
  shorttitle = {An Introduction to Statistical Learning},
  author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert and Taylor, Jonathan E.},
  year = {2023},
  series = {Springer Texts in Statistics},
  publisher = {Springer},
  address = {Cham},
  isbn = {978-3-031-38746-3 978-3-031-38747-0},
  langid = {english}
}

@misc{kosourovaHowCreateDataframe2025,
  title = {How to {{Create}} a {{Dataframe}} in {{R}} (with 30 {{Code Examples}})},
  author = {Kosourova, Elena},
  year = {2025},
  month = mar,
  journal = {Dataquest},
  urldate = {2025-04-02},
  abstract = {Dataframes are essential data structures in the R programming language. In this tutorial, we'll discuss how to create a dataframe in R.},
  howpublished = {https://www.dataquest.io/blog/how-to-create-a-dataframe-in-r/},
  langid = {american},
  file = {C:\Users\Daniel\Zotero\storage\QTW4UQD4\how-to-create-a-dataframe-in-r.html}
}

@misc{kuldipAnswerConvertCategorical2020,
  title = {Answer to "{{Convert}} Categorical Variables to Numeric in {{R}}"},
  author = {Kuldip},
  year = {2020},
  month = sep,
  journal = {Stack Overflow},
  urldate = {2025-03-23},
  file = {C:\Users\Daniel\Zotero\storage\NTTKD7WW\convert-categorical-variables-to-numeric-in-r.html}
}

@misc{Loop,
  title = {R {{For Loop}}},
  urldate = {2025-04-23},
  abstract = {W3Schools offers free online tutorials, references and exercises in all the major languages of the web. Covering popular subjects like HTML, CSS, JavaScript, Python, SQL, Java, and many, many more.},
  howpublished = {https://www.w3schools.com/r/r\_for\_loop.asp},
  langid = {american},
  file = {C:\Users\Daniel\Zotero\storage\79PNKARQ\r_for_loop.html}
}

@misc{MclustModelNamesMCLUSTModel,
  title = {{{mclustModelNames}}: {{MCLUST Model Names}} in Mclust: {{Gaussian Mixture Modelling}} for {{Model-Based Clustering}}, {{Classification}}, and {{Density Estimation}}},
  shorttitle = {{{mclustModelNames}}},
  urldate = {2025-04-26},
  howpublished = {https://rdrr.io/cran/mclust/man/mclustModelNames.html},
  langid = {english},
  file = {C:\Users\Daniel\Zotero\storage\ZPJ8I9QY\mclustModelNames.html}
}

@misc{mphHowCombineTwo2024,
  title = {How to {{Combine Two Data Frames}} in {{R}} with {{Different Columns Using Base R}}, Dplyr, and Data.Table -- {{Steve}}'s {{Data Tips}} and {{Tricks}}},
  author = {MPH, Steven P. Sanderson II},
  year = {2024},
  month = oct,
  journal = {Steve's Data Tips and Tricks},
  urldate = {2025-04-02},
  abstract = {Combine data frames in R with different columns using base R, dplyr, and data.table. Detailed guide for beginner R programmers with practical examples and code. Optimize your data manipulation skills.},
  howpublished = {https://www.spsanderson.com/steveondata/posts/2024-10-10/},
  langid = {english},
  file = {C:\Users\Daniel\Zotero\storage\EM95TVJJ\2024-10-10.html}
}

@misc{MultinomialLogisticRegression18:24:37+00:00,
  title = {Multinomial {{Logistic Regression}} in {{R}}},
  year = {18:24:37+00:00},
  journal = {GeeksforGeeks},
  urldate = {2025-04-23},
  abstract = {Your All-in-One Learning Portal: GeeksforGeeks is a comprehensive educational platform that empowers learners across domains-spanning computer science and programming, school education, upskilling, commerce, software tools, competitive exams, and more.},
  chapter = {R Language},
  howpublished = {https://www.geeksforgeeks.org/multinomial-logistic-regression-in-r/},
  langid = {american},
  file = {C:\Users\Daniel\Zotero\storage\NHHGS4N9\multinomial-logistic-regression-in-r.html}
}

@misc{nghauranAnswerConvertCategorical2017,
  title = {Answer to "{{Convert}} Categorical Variables to Numeric in {{R}}"},
  author = {{nghauran}},
  year = {2017},
  month = dec,
  journal = {Stack Overflow},
  urldate = {2025-03-23},
  file = {C:\Users\Daniel\Zotero\storage\7JYQL5JQ\convert-categorical-variables-to-numeric-in-r.html}
}

@misc{nickHowFindStatistical2020,
  type = {Forum Post},
  title = {How to Find the Statistical Mode?},
  author = {Nick},
  year = {2020},
  month = aug,
  journal = {Stack Overflow},
  urldate = {2025-04-26},
  file = {C:\Users\Daniel\Zotero\storage\88JG5NT2\how-to-find-the-statistical-mode.html}
}

@misc{PDFBasics,
  title = {{{PDF Basics}}},
  journal = {Quarto},
  urldate = {2025-04-29},
  howpublished = {https://quarto.org/docs/output-formats/pdf-basics.html\#raw-latex},
  langid = {english},
  file = {C:\Users\Daniel\Zotero\storage\8RBGHZSY\pdf-basics.html}
}

@misc{PDFPowerOutliers,
  title = {({{PDF}}) {{The Power}} of {{Outliers}} (and {{Why Researchers Should Always Check}} for {{Them}})},
  journal = {ResearchGate},
  urldate = {2025-04-29},
  abstract = {PDF {\textbar} There has been much debate in the literature regarding what to do with extreme or influential data points. The goal of this paper is to summarize... {\textbar} Find, read and cite all the research you need on ResearchGate},
  howpublished = {https://www.researchgate.net/publication/242073851\_The\_Power\_of\_Outliers\_and\_Why\_Researchers\_Should\_Always\_Check\_for\_Them},
  langid = {english},
  file = {C:\Users\Daniel\Zotero\storage\6K3JT3DE\242073851_The_Power_of_Outliers_and_Why_Researchers_Should_Always_Check_for_Them.html}
}

@misc{RstatixDocumentation,
  title = {Rstatix Documentation},
  urldate = {2025-04-21},
  abstract = {The rstatix package contains the following man pages: add\_significance adjust\_pvalue anova\_summary anova\_test as\_cor\_mat binom\_test box\_m chisq\_test cochran\_qtest cohens\_d cor\_as\_symbols cor\_mark\_significant cor\_mat cor\_plot cor\_reorder cor\_reshape cor\_select cor\_test counts\_to\_cases cramer\_v df\_arrange df\_get\_var\_names df\_group\_by df\_label\_value df\_nest\_by df\_select df\_split\_by df\_unite doo dunn\_test emmeans\_test eta\_squared factorial\_design factors fisher\_test freq\_table friedman\_effsize friedman\_test games\_howell\_test get\_comparisons get\_mode get\_pvalue\_position get\_summary\_stats get\_test\_label kruskal\_effsize kruskal\_test levene\_test mahalanobis\_distance make\_clean\_names Manova mcnemar\_test multinom\_test outliers pipe prop\_test prop\_trend\_test pull\_triangle p\_value reexports remove\_ns replace\_triangle sample\_n\_by shapiro\_test sign\_test t\_test tukey\_hsd welch\_anova\_test wilcox\_effsize wilcox\_test},
  howpublished = {https://rdrr.io/cran/rstatix/man/},
  langid = {english},
  file = {C:\Users\Daniel\Zotero\storage\L9T3JICT\man.html}
}

@misc{seanosapienAnswerConvertCategorical2019,
  title = {Answer to "{{Convert}} Categorical Variables to Numeric in {{R}}"},
  author = {Seanosapien},
  year = {2019},
  month = jun,
  journal = {Stack Overflow},
  urldate = {2025-03-23},
  file = {C:\Users\Daniel\Zotero\storage\9EQZS8X4\convert-categorical-variables-to-numeric-in-r.html}
}

@misc{TidyversePackages,
  title = {Tidyverse Packages},
  urldate = {2025-03-23},
  abstract = {Installation and use       Install all the packages in the tidyverse by running install.packages("tidyverse").   Run library(tidyverse) to load the core tidyverse and make it available in your current R session.   Learn more about the tidyverse package at https://tidyverse.tidyverse.org. Core tidyverse     The core tidyverse includes the packages that you're likely to use in everyday data analyses. As of tidyverse 1.},
  howpublished = {https://www.tidyverse.org/packages/},
  langid = {american},
  file = {C:\Users\Daniel\Zotero\storage\RS3CJEYQ\packages.html}
}

@misc{vincentAnswerConvertCategorical2017,
  title = {Answer to "{{Convert}} Categorical Variables to Numeric in {{R}}"},
  author = {Vincent},
  year = {2017},
  month = dec,
  journal = {Stack Overflow},
  urldate = {2025-03-23},
  file = {C:\Users\Daniel\Zotero\storage\XGZ622P5\convert-categorical-variables-to-numeric-in-r.html}
}

@misc{WeightedLogisticRegression09:33:43+00:00,
  title = {Weighted Logistic Regression in {{R}}},
  year = {09:33:43+00:00},
  journal = {GeeksforGeeks},
  urldate = {2025-04-23},
  abstract = {Your All-in-One Learning Portal: GeeksforGeeks is a comprehensive educational platform that empowers learners across domains-spanning computer science and programming, school education, upskilling, commerce, software tools, competitive exams, and more.},
  chapter = {Machine Learning},
  howpublished = {https://www.geeksforgeeks.org/weighted-logistic-regression-in-r/},
  langid = {american},
  file = {C:\Users\Daniel\Zotero\storage\9D5WP2UA\weighted-logistic-regression-in-r.html}
}

@misc{williamsAnswerHowFind2011,
  title = {Answer to "{{How}} to Find the Statistical Mode?"},
  shorttitle = {Answer to "{{How}} to Find the Statistical Mode?},
  author = {Williams, Ken},
  year = {2011},
  month = nov,
  journal = {Stack Overflow},
  urldate = {2025-04-26},
  file = {C:\Users\Daniel\Zotero\storage\QSV5FU58\how-to-find-the-statistical-mode.html}
}

@misc{wrightHopkinsCalculateHopkins2022,
  title = {Hopkins: {{Calculate Hopkins Statistic}} for {{Clustering}}},
  shorttitle = {Hopkins},
  author = {Wright, Kevin},
  year = {2022},
  month = jan,
  pages = {1.1},
  publisher = {Comprehensive R Archive Network},
  doi = {10.32614/CRAN.package.hopkins},
  urldate = {2025-04-02},
  abstract = {Calculate Hopkins statistic to assess the clusterability of data. See Wright (2023) {$<$}doi:10.32614/RJ-2022-055{$>$}.},
  langid = {english},
  file = {C:\Users\Daniel\Zotero\storage\LLG5XFVT\Wright - 2022 - hopkins Calculate Hopkins Statistic for Clustering.pdf}
}

@article{wrightWillRealHopkins2022,
  title = {Will the {{Real Hopkins Statistic Please Stand Up}}?},
  author = {Wright, Kevin},
  year = {2022},
  month = dec,
  journal = {The R Journal},
  volume = {14},
  number = {3},
  pages = {282--292},
  issn = {2073-4859},
  doi = {10.32614/RJ-2022-055},
  urldate = {2025-04-25},
  abstract = {Hopkins statistic (Hopkins and Skellam 1954) can be used to test for spatial randomness of data and for detecting clusters in data. Although the method is nearly 70 years old, there is persistent confusion regarding the definition and calculation of the statistic. We investigate the confusion and its possible origin. Using the most general definition of Hopkins statistic, we perform a small simulation to verify its distributional properties, provide a visualization of how the statistic is calculated, and provide a fast R function to correctly calculate the statistic. Finally, we propose a protocol of five questions to guide the use of Hopkins statistic.},
  langid = {english},
  file = {C:\Users\Daniel\Zotero\storage\BPNWLK52\Wright - 2022 - Will the Real Hopkins Statistic Please Stand Up.pdf}
}
