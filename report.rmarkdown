---
title: "Advanced Statistics: Application of supervised and unsupervised methods to biological data"
author: "Daniel Hill"
date: "27 April 2025"
format: 
  pdf:
    documentclass: scrartcl
    # classoption: twocolumn
    number-sections: true
    toc: true
    toc-depth: 3
    toc-title: Contents
    lof: true
    lot: true
    geometry:
      - top=30mm
      - left=20mm
      - heightrounded
    dev: pdf
    natbiboptions: "authoryear,round"
    include-in-header:
      text: |
        \usepackage{etoolbox}
        \pretocmd{\tableofcontents}{\newpage}{}{}
abstract: "**Abstract:** Biological data with twenty features and four categorical class labels is explored and analysed using advanced statistical techniques in this report. Both supervised and unsupervised methods were implemented and evaluated, and the broad selection of models includes logistic regression, support vector machine, random forest, agglomerative hierarchical clustering, and gaussian mixture modelling. Comparing the results achieved using a selection of models with different underlying principles gives insight into the nature of the data. For example, the success of model-based clustering compared to hierarchical clustering and tree-based learning suggests the lack of hierarchy among the categorical labels, and the success of factor analysis as a dimensionality reduction technique suggests the presence of underlying biological mechanisms leading to several of the features arising together. Models achieving over 90% accuracy were produced, but all models performed notably worse at separating one of the categories that overlapped the other three."
bibliography: references.bib
editor: visual
include-in-header:
  text: |
    \usepackage{titling}
    \pretitle{\begin{center}\LARGE\bfseries}
    \posttitle{\end{center}}
    \preauthor{\begin{center}\large}
    \postauthor{\end{center}}
    \predate{\begin{center}}
    \postdate{\end{center}}
---


\newpage



```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,        
  warning = FALSE,     
  message = FALSE,
  fig.align = "center",
  fig.width = 6,
  fig.height = 4,
  out.width = "80%"
)
```

```{r}
library(tidyverse)
library(carat)
library(skimr)
library(kableExtra)
library(reshape2)
library(viridis)
library(hopkins)
```



# Introduction

this is a section where i write the introduction. 


# Methods

## Data Description
20 features, a label with four catagorical classes. two groups of correlated features. Outliers removed using z score method. one feature transformed using logarithm. All features scaled and centered. All features numeric. 

Bootstrapping was used to create a larger dataset.


## Exploratory Data Analysis Approach

find distributions within each feature, look for correlations between features, scatter between plots that features that have high correlation to the catagorical label or another feature, use PCA to visualize all data together, calculate hopkins statistic to determine the clustering tendency of the data



```{r feature-data-descriptions}
data <- read_csv("Data(2).csv")
desc <- skim(data)
desc %>% 
  select(
    skim_variable, 
    n_missing, 
    numeric.mean, 
    numeric.sd, 
    numeric.p0, 
    numeric.p25, 
    numeric.p50, 
    numeric.p75, 
    numeric.p100, 
  ) %>% 
  filter(
    skim_variable != "label"
  ) %>%
  rename(
    "Variable Name" = skim_variable, 
    "No. missing values" = n_missing, 
    "mean" = numeric.mean, 
    "Std deviation" = numeric.sd, 
    "min" = numeric.p0, 
    "25th %ile" = numeric.p25, 
    "median" = numeric.p50, 
    "75th %ile" = numeric.p75, 
    "max" = numeric.p100, 
  ) %>%
  mutate(across(everything(), ~ifelse(. == "NA", "-", .))) %>%
  kable(
    caption = "Feature Descriptions", 
    digits = 3, 
    align = "c",
    booktabs = TRUE,
    na="-"
  ) %>% 
  kable_styling(
    latex_options = c("striped", "hold_position", "scale_down"),
    font_size = 9
  )
```

```{r correlation-heatmap, fig.cap="**Feature and Class Correlation Matrix:** *highlighting relationships between variables and relationships with catagorical labels*"}
one_hot_data <- model.matrix(~ label - 1, data = data)
one_hot_data <- data %>%
  select(-label) %>%
  bind_cols(one_hot_data)

cor_matrix <- cor(one_hot_data, use = "complete.obs", method = "pearson")
cor_data <- melt(cor_matrix)

ggplot(cor_data, aes(x = Var1, y = Var2, fill = value)) +
  geom_tile(
    color = "white", 
    size = 0.25
  ) +
  scale_fill_gradient2(
    low = "darkblue",
    high = "darkred",
    mid = "white",
    name = "Correlation",
    limit = c(-1, 1), 
    guide = guide_colorbar(barwidth = 1, barheight = 10)
  ) + 
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 60, vjust = 0.75, hjust = 0.5, size = 7),  # all the feature names overlap if horisontal
    axis.text.y = element_text(size = 7),
    axis.title = element_blank(),           # axis titles arent meaningful
    panel.grid = element_blank(),       
    aspect.ratio = 1,                       # make plot square
    legend.position = "right",
    plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
    plot.subtitle = element_text(hjust = 0.5, size = 10)
  ) +
  coord_fixed()
```

```{r column-histograms, fig.cap="**Distributions within feature columns:** *histograms showing scale and skewness of data*"}
data_long <- drop_na(data) %>%
  select(- starts_with("label")) %>%
  pivot_longer(
    cols = everything(), 
    names_to = "variable", 
    values_to = "value"
  ) %>%
  mutate(
    dist_group = case_when(
      variable == "X7" ~ "Bimodal",
      variable == "X8" ~ "Skewed",
      TRUE ~ "Normal"
    )
  )

ggplot(data_long, aes(x = value, fill = dist_group)) +
  geom_histogram(
    bins = 50, 
    alpha = 0.9,
    size = 0.2
  ) +
  scale_fill_manual(
    values = c("Normal" = "skyblue", "Bimodal" = "pink", "Skewed" = "purple"),
    name = "Distribution"
  ) +
  facet_wrap(
    ~ variable, 
    scales = "free"
    ) +
  theme_minimal() +
  labs(
    x = "Value", 
    y = "Count", 
    title = "Histograms of All Columns"
  )

```

```{r class-boxplots, fig.cap="**Distributions within feature columns:** *Boxplots by class label by feature*"}
data_clean <- data %>% drop_na()
clean_and_long <- pivot_longer(data_clean, cols = 1:20, names_to= "Feature", values_to = "Value")
boxplot_data <- clean_and_long %>%
  mutate(
    important = case_when(
      Feature %in% c("X7", "X8", "X9", "X11", "X3", "X2") ~ "1",
      Feature %in% c("X20", "X19", "X18", "X17") ~ "-1",
      TRUE ~ "0"
    )
  )

boxplot <- ggplot( boxplot_data,  aes(label, Value, colour=label)  ) +
  geom_boxplot(
    outlier.shape = 21,
    outlier.size = 1,
    outlier.alpha = 0.5,
    width = 0.7
  ) +
  facet_wrap(~Feature, scales="free_y") +
  theme_minimal() +
  theme(
    strip.text = element_text(face = "bold", size = 10),
    panel.spacing = unit(0.5, "lines"),
    legend.position = "right",
    axis.text.x = element_text(angle = 0, size = 8),
    axis.text.y = element_blank(),
    axis.title.y = element_blank(),
    axis.title.x = element_blank(),
    panel.grid.minor = element_blank(),
    panel.grid.major.x = element_blank(),
    # panel.border = element_rect(
    #   colour = case_when(
    #     boxplot_data$important == "1" ~ "red",
    #     boxplot_data$important == "-1" ~ "lightblue",
    #     boxplot_data$important == "0" ~ "white"
    #   ),
    #   fill = NA,
    #   size = 0.5
    # )
  ) +
  geom_rect(data = subset(boxplot_data, important == "1") %>% 
              distinct(Feature, important),
            aes(xmin = -Inf, xmax = Inf, ymin = -Inf, ymax = Inf),
            color = "red", fill = NA, size = 0.5, alpha = 0.8,
            inherit.aes = FALSE) +
  geom_rect(data = subset(boxplot_data, important == "-1") %>% 
              distinct(Feature, important),
            aes(xmin = -Inf, xmax = Inf, ymin = -Inf, ymax = Inf),
            color = "blue", fill = NA, size = 0.5, alpha = 0.8,
            inherit.aes = FALSE)

boxplot
```

```{r pca-plot, fig.cap="**PCA plotting of class labels:** *scatterplot showing clustering tendency of catagorical classes*"}

data_clean <- data %>% drop_na() # pca wont work with null values in the frame
pc <- prcomp(data_clean[1:20],
            center = TRUE,
            scale = TRUE)
pc_data <- data.frame(pc$x)

labelled_pca <- bind_cols(pc_data[1:2], data.frame(data_clean$label), .name_repair = "universal")

ggplot(labelled_pca, aes(x=PC1, y=PC2, colour=data_clean.label)) +
      geom_point(
        alpha=0.4
        ,size=1
      ) +
      labs(
          colour="Class Label"
        ) +
  geom_density2d(alpha=0.75)
    
```

```{r hopkins}
data_clean <- read_csv("data_clean.csv") %>% mutate(label = as.factor(label))

hopkins_stat <- hopkins(data_clean[0:20], m = nrow(data_clean)/10) 
# we leave one row out to be the 'reference point' that we measure the distance to the other points from. i think. 

data_unlabelled <- data_clean %>% select(-label)
hopkins_stat_unlabelled <- hopkins(data_unlabelled[0:19], m = nrow(data_unlabelled)/10)

binary_data <- data_clean %>% mutate(label = factor( ifelse(label == "D", 1, 0), levels = c(0, 1) ))
binary_hopkins_stat <- hopkins(binary_data[0:20], m = nrow(binary_data)/10)

correlated_data <- data_clean %>%
  select(
    X2,
    X3,
    X7,
    X8,
    X9,
    X11
  )
hopkins_stat_high_diffs <- hopkins(correlated_data, m = nrow(correlated_data)/10) 


least_correlated_data <- data_clean %>%
  select(
    X17,
    X18,
    X19,
    X20
  )
hopkins_stat_low_corr <- hopkins(least_correlated_data, m = nrow(least_correlated_data)/10) 

boring_data <- data_clean %>%
  select(
    X1,
    X4,
    X6,
    X10,
    X12,
    X13,
    X14,
    X15,
    X16
  )
hopkins_stat_boring <- hopkins(boring_data, m = nrow(boring_data)/10)

# Print result
# print(hopkins_stat)
# print(hopkins_stat_unlabelled)
# print(binary_hopkins_stat)
# print(hopkins_stat_high_diffs)
# print(hopkins_stat_low_corr)
# print(hopkins_stat_boring)

names = c("All Features + Label", "All Features with Label Removed", "All Features Binary Class D vs Rest", "Features X2,X3,X7,X8,X9,X11", "Features X17,X18,X19,X20", "Features X1,X4,X6,X10,X12,X13,X14,X15,X16")
# print(names)
scores = c(hopkins_stat, hopkins_stat_unlabelled, binary_hopkins_stat, hopkins_stat_high_diffs, hopkins_stat_low_corr, hopkins_stat_boring)
# print(scores)
df <- data.frame(names, scores) %>% 
  rename(
    "Columns used" = names,
    "Hopkins Statistic Score" = scores
  )

hopkins_table <- df %>%
  kable(
    caption = "Hopkins Statistic Scores", 
    digits = 7, 
    align = "l",
    booktabs = TRUE,
    na="-"
  ) %>% 
  kable_styling(
    latex_options = c("striped", "hold_position", "scale_down"),
    font_size = 9
  )

hopkins_table
```








## Supervised Learning Methods

logistic regression - including class weighting and L2 regularization, and feature selection. Random forest, including feature selection and tuning of mtry. SVM, with feature selection and tuning of kernel selection, gamma and cost parameters.


## Unsupervised Learning Methods

Agglomerative hierarchical clustering, including tuning of linking metric. Gaussian mixture model based clustering, including selecting a model, regularization using shrinkage parameter, and dimensionality reduction using factor analysis.    


# Results

## Exploratory Data Analysis Findings

most features are normally distributed except for X8 which is highly skewed. Features originally had different scales. X7, X8, X9 columns are correlated and correlate highly with the labels. X17, X18, X19 and X20 are highly correlated together and have very low correlation with the labels. 

The PCA showed the four labels had some clustered structure, but also some significant overlap. The hopkins statistic showed there was a moderate clustering tendency, but that the label column when included made the clustering tendency extremely high. This is an initial suggestion that supervised learning would be more effective than unsupervised learning


## Supervised Learning Results

logistic regression was not great. weighting did nothing, as expected. regularization didn't really do anything. feature selection did not improve the model. We saw that the model particularly underperformed at classifying class D correctly.

The random forest performed well without any configuration. feature selection was not effective. still struggled at seperating class D. mtry was tuned. 

svm was good but not as good as random forest. lots of tuning. feature selection was uneffective


## Unsupervised Learning Results

ahc was good not great.

Gaussian mixed model clustering performed very well. dimensionality reduction using factor analysis was slightly effective. 

# Discussion

the final model had good overall accuracy but caution is advised when using a ml model with this data due to the poor performance in class D - if false positives or false negatives in this class have serious implications, some models become immediately unusable. 



# Conclusion



# References



