---
title: "Advanced Statistics: Application of supervised and unsupervised methods to biological data"
author: "Daniel Hill"
date: "27 April 2025"
format: 
  pdf:
    documentclass: scrartcl
    # classoption: twocolumn
    number-sections: true
    toc: true
    toc-depth: 3
    toc-title: Contents
    lof: true
    lot: true
    geometry:
      - top=30mm
      - left=20mm
      - heightrounded
    dev: pdf
    natbiboptions: "authoryear,round"
    include-in-header:
      text: |
        \usepackage{etoolbox}
        \pretocmd{\tableofcontents}{\newpage}{}{}
abstract: "**Abstract:** Biological data with twenty features and four categorical class labels is explored and analysed using advanced statistical techniques in this report. Both supervised and unsupervised methods were implemented and evaluated, and the broad selection of models includes logistic regression, support vector machine, random forest, agglomerative hierarchical clustering, and gaussian mixture modelling. Comparing the results achieved using a selection of models with different underlying principles gives insight into the nature of the data. For example, the success of model-based clustering compared to hierarchical clustering and tree-based learning suggests the lack of hierarchy among the categorical labels, and the success of factor analysis as a dimensionality reduction technique suggests the presence of underlying biological mechanisms leading to several of the features arising together. Models achieving over 90% accuracy were produced, but all models performed notably worse at separating one of the categories that overlapped the other three."
bibliography: references.bib
editor: visual
include-in-header:
  text: |
    \usepackage{titling}
    \pretitle{\begin{center}\LARGE\bfseries}
    \posttitle{\end{center}}
    \preauthor{\begin{center}\large}
    \postauthor{\end{center}}
    \predate{\begin{center}}
    \postdate{\end{center}}
---
\newpage

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,        
  warning = FALSE,     
  message = FALSE,
  fig.align = "center",
  fig.width = 6,
  fig.height = 4,
  out.width = "80%"
)
```
```{r load-libraries}
library(tidyverse)
library(carat)
library(caret)
library(skimr)
library(kableExtra)
library(reshape2)
library(viridis)
library(hopkins)
library(mnet)
library(nnet)
library(pROC)
library(randomForest)
```

# Introduction

this is a section where i write the introduction. 


# Methods

## Data Description
20 features, a label with four catagorical classes. two groups of correlated features. Outliers removed using z score method. one feature transformed using logarithm. All features scaled and centered. All features numeric. 

Bootstrapping was used to create a larger dataset.


## Exploratory Data Analysis Approach

find distributions within each feature, look for correlations between features, scatter between plots that features that have high correlation to the catagorical label or another feature, use PCA to visualize all data together, calculate hopkins statistic to determine the clustering tendency of the data

```{r feature-data-descriptions}
data <- read_csv("Data(2).csv")
desc <- skim(data)
desc %>% 
  select(
    skim_variable, 
    n_missing, 
    numeric.mean, 
    numeric.sd, 
    numeric.p0, 
    numeric.p25, 
    numeric.p50, 
    numeric.p75, 
    numeric.p100, 
  ) %>% 
  filter(
    skim_variable != "label"
  ) %>%
  rename(
    "Variable Name" = skim_variable, 
    "No. missing values" = n_missing, 
    "mean" = numeric.mean, 
    "Std deviation" = numeric.sd, 
    "min" = numeric.p0, 
    "25th %ile" = numeric.p25, 
    "median" = numeric.p50, 
    "75th %ile" = numeric.p75, 
    "max" = numeric.p100, 
  ) %>%
  mutate(across(everything(), ~ifelse(. == "NA", "-", .))) %>%
  kable(
    caption = "Feature Descriptions", 
    digits = 3, 
    align = "c",
    booktabs = TRUE,
    na="-"
  ) %>% 
  kable_styling(
    latex_options = c("striped", "hold_position", "scale_down"),
    font_size = 9
  )
```

```{r correlation-heatmap, fig.cap="**Feature and Class Correlation Matrix:** *highlighting relationships between variables and relationships with catagorical labels*"}
one_hot_data <- model.matrix(~ label - 1, data = data)
one_hot_data <- data %>%
  select(-label) %>%
  bind_cols(one_hot_data)

cor_matrix <- cor(one_hot_data, use = "complete.obs", method = "pearson")
cor_data <- melt(cor_matrix)

ggplot(cor_data, aes(x = Var1, y = Var2, fill = value)) +
  geom_tile(
    color = "white", 
    size = 0.25
  ) +
  scale_fill_gradient2(
    low = "darkblue",
    high = "darkred",
    mid = "white",
    name = "Correlation",
    limit = c(-1, 1), 
    guide = guide_colorbar(barwidth = 1, barheight = 10)
  ) + 
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 60, vjust = 0.75, hjust = 0.5, size = 7),  # all the feature names overlap if horisontal
    axis.text.y = element_text(size = 7),
    axis.title = element_blank(),           # axis titles arent meaningful
    panel.grid = element_blank(),       
    aspect.ratio = 1,                       # make plot square
    legend.position = "right",
    plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
    plot.subtitle = element_text(hjust = 0.5, size = 10)
  ) +
  coord_fixed()
```


```{r column-histograms, fig.cap="**Distributions within feature columns:** *histograms showing scale and skewness of data*"}
data_long <- drop_na(data) %>%
  select(- starts_with("label")) %>%
  pivot_longer(
    cols = everything(), 
    names_to = "variable", 
    values_to = "value"
  ) %>%
  mutate(
    dist_group = case_when(
      variable == "X7" ~ "Bimodal",
      variable == "X8" ~ "Skewed",
      TRUE ~ "Normal"
    )
  )

ggplot(data_long, aes(x = value, fill = dist_group)) +
  geom_histogram(
    bins = 50, 
    alpha = 0.9,
    size = 0.2
  ) +
  scale_fill_manual(
    values = c("Normal" = "skyblue", "Bimodal" = "pink", "Skewed" = "purple"),
    name = "Distribution"
  ) +
  facet_wrap(
    ~ variable, 
    scales = "free"
    ) +
  theme_minimal() +
  labs(
    x = "Value", 
    y = "Count", 
    title = "Histograms of All Columns"
  )

```
```{r class-boxplots, fig.cap="**Distributions within feature columns:** *Boxplots by class label by feature*"}
data_clean <- data %>% drop_na()
clean_and_long <- pivot_longer(data_clean, cols = 1:20, names_to= "Feature", values_to = "Value")
boxplot_data <- clean_and_long %>%
  mutate(
    important = case_when(
      Feature %in% c("X7", "X8", "X9", "X11", "X3", "X2") ~ "1",
      Feature %in% c("X20", "X19", "X18", "X17") ~ "-1",
      TRUE ~ "0"
    )
  )

boxplot <- ggplot( boxplot_data,  aes(label, Value, colour=label)  ) +
  geom_boxplot(
    outlier.shape = 21,
    outlier.size = 1,
    outlier.alpha = 0.5,
    width = 0.7
  ) +
  facet_wrap(~Feature, scales="free_y") +
  theme_minimal() +
  theme(
    strip.text = element_text(face = "bold", size = 10),
    panel.spacing = unit(0.5, "lines"),
    legend.position = "right",
    axis.text.x = element_text(angle = 0, size = 8),
    axis.text.y = element_blank(),
    axis.title.y = element_blank(),
    axis.title.x = element_blank(),
    panel.grid.minor = element_blank(),
    panel.grid.major.x = element_blank(),
    # panel.border = element_rect(
    #   colour = case_when(
    #     boxplot_data$important == "1" ~ "red",
    #     boxplot_data$important == "-1" ~ "lightblue",
    #     boxplot_data$important == "0" ~ "white"
    #   ),
    #   fill = NA,
    #   size = 0.5
    # )
  ) +
  geom_rect(data = subset(boxplot_data, important == "1") %>% 
              distinct(Feature, important),
            aes(xmin = -Inf, xmax = Inf, ymin = -Inf, ymax = Inf),
            color = "red", fill = NA, size = 0.5, alpha = 0.8,
            inherit.aes = FALSE) +
  geom_rect(data = subset(boxplot_data, important == "-1") %>% 
              distinct(Feature, important),
            aes(xmin = -Inf, xmax = Inf, ymin = -Inf, ymax = Inf),
            color = "blue", fill = NA, size = 0.5, alpha = 0.8,
            inherit.aes = FALSE)

boxplot
```


```{r pca-plot, fig.cap="**PCA plotting of class labels:** *scatterplot showing clustering tendency of catagorical classes*"}

data_clean <- data %>% drop_na() # pca wont work with null values in the frame
pc <- prcomp(data_clean[1:20],
            center = TRUE,
            scale = TRUE)
pc_data <- data.frame(pc$x)

labelled_pca <- bind_cols(pc_data[1:2], data.frame(data_clean$label), .name_repair = "universal")

ggplot(labelled_pca, aes(x=PC1, y=PC2, colour=data_clean.label)) +
      geom_point(
        alpha=0.4
        ,size=1
      ) +
      labs(
          colour="Class Label"
        ) +
  geom_density2d(alpha=0.75)
    
```

```{r hopkins}
data_clean <- read_csv("data_clean.csv") %>% mutate(label = as.factor(label))

hopkins_stat <- hopkins(data_clean[0:20], m = nrow(data_clean)/10) 
# we leave one row out to be the 'reference point' that we measure the distance to the other points from. i think. 

data_unlabelled <- data_clean %>% select(-label)
hopkins_stat_unlabelled <- hopkins(data_unlabelled[0:19], m = nrow(data_unlabelled)/10)

binary_data <- data_clean %>% mutate(label = factor( ifelse(label == "D", 1, 0), levels = c(0, 1) ))
binary_hopkins_stat <- hopkins(binary_data[0:20], m = nrow(binary_data)/10)

correlated_data <- data_clean %>%
  select(
    X2,
    X3,
    X7,
    X8,
    X9,
    X11
  )
hopkins_stat_high_diffs <- hopkins(correlated_data, m = nrow(correlated_data)/10) 


least_correlated_data <- data_clean %>%
  select(
    X17,
    X18,
    X19,
    X20
  )
hopkins_stat_low_corr <- hopkins(least_correlated_data, m = nrow(least_correlated_data)/10) 

boring_data <- data_clean %>%
  select(
    X1,
    X4,
    X6,
    X10,
    X12,
    X13,
    X14,
    X15,
    X16
  )
hopkins_stat_boring <- hopkins(boring_data, m = nrow(boring_data)/10)

# Print result
# print(hopkins_stat)
# print(hopkins_stat_unlabelled)
# print(binary_hopkins_stat)
# print(hopkins_stat_high_diffs)
# print(hopkins_stat_low_corr)
# print(hopkins_stat_boring)

names = c("All Features + Label", "All Features with Label Removed", "All Features Binary Class D vs Rest", "Features X2,X3,X7,X8,X9,X11", "Features X17,X18,X19,X20", "Features X1,X4,X6,X10,X12,X13,X14,X15,X16")
# print(names)
scores = c(hopkins_stat, hopkins_stat_unlabelled, binary_hopkins_stat, hopkins_stat_high_diffs, hopkins_stat_low_corr, hopkins_stat_boring)
# print(scores)
df <- data.frame(names, scores) %>% 
  rename(
    "Columns used" = names,
    "Hopkins Statistic Score" = scores
  )

hopkins_table <- df %>%
  kable(
    caption = "Hopkins Statistic Scores", 
    digits = 7, 
    align = "lc", # left align the column names, but centre the scores
    booktabs = TRUE,
    na="-"
  ) %>% 
  kable_styling(
    latex_options = c("striped", "hold_position", "scale_down"),
    font_size = 9
  )

hopkins_table
```






## Supervised Learning Methods

logistic regression - including class weighting and L2 regularization, and feature selection. Random forest, including feature selection and tuning of mtry. SVM, with feature selection and tuning of kernel selection, gamma and cost parameters.


## Unsupervised Learning Methods

Agglomerative hierarchical clustering, including tuning of linking metric. Gaussian mixture model based clustering, including selecting a model, regularization using shrinkage parameter, and dimensionality reduction using factor analysis.    


# Results

## Exploratory Data Analysis Findings

most features are normally distributed except for X8 which is highly skewed. Features originally had different scales. X7, X8, X9 columns are correlated and correlate highly with the labels. X17, X18, X19 and X20 are highly correlated together and have very low correlation with the labels. 

The PCA showed the four labels had some clustered structure, but also some significant overlap. The hopkins statistic showed there was a moderate clustering tendency, but that the label column when included made the clustering tendency extremely high. This is an initial suggestion that supervised learning would be more effective than unsupervised learning


## Supervised Learning Results

### Logistic Regression

```{r simple-logistic-regression-confusion-matrix}

data <- read_csv("data_clean.csv")
data$label <- as.factor(data$label) # factor is like an enum, means A,B,C,D are read as categories not strings
set.seed(121) # set the seed at the top so the entire doc is reproducable
# create test and train data splits we can use for the rest of the modelling
training.samples <- data$label %>% 
  createDataPartition(p = 0.8, list = FALSE) # create 80:20 split for train:test
train_data  <- data[training.samples, ]
test_data <- data[-training.samples, ]

simple_logreg_model <- multinom(
    label ~ ., # use all cols
    data = train_data,
    trace = FALSE
    )
simple_logreg_predictions <- predict(simple_logreg_model, test_data, type = "class")
simple_logreg_conf_matrix = confusionMatrix(simple_logreg_predictions, test_data$label)
simple_logreg_conf_df <- as.data.frame(simple_logreg_conf_matrix$table)

simple_logreg_conf_df <- simple_logreg_conf_df %>% pivot_wider(names_from = Reference, values_from = Freq)

simp_logreg_confm_table <- simple_logreg_conf_df %>%
  kable(
    caption = "Simple Logistic Regression Confusion Matrix", 
    digits = 7, 
    align = "c",
    booktabs = TRUE,
    na="-"
  ) %>% 
  kable_styling(
    latex_options = c("striped", "hold_position", "scale_down"),
    font_size = 9
  ) %>%
  add_header_above(
    c(" " = 1, "Reference" = 4)
  ) #%>% 
  # pack_rows(
  #   "Prediction",
  #   1,
  #   4
  # )

simp_logreg_confm_table


simp_logreg_overall_stats <- data.frame(
  Statistic = names(simple_logreg_conf_matrix$overall),
  Value = simple_logreg_conf_matrix$overall
)


simp_logreg_overall_stats_table <- simp_logreg_overall_stats %>%
  kable(
    caption = "Simple Logistic Regression Overall Statistics",
    col.names = c("Statistic", "Value"),
    digits = 4,
    align = "lc",
    booktabs = TRUE
  ) %>%
  kable_styling(
    latex_options = c("striped", "hold_position"),
    font_size = 9
  )

simp_logreg_overall_stats_table



simp_logreg_byClass_stats <- as.data.frame(simple_logreg_conf_matrix$byClass)
simp_logreg_byClass_stats <- t(simp_logreg_byClass_stats) # I want the class names to be columns not rows
simp_logreg_byClass_stats <- as.data.frame(simp_logreg_byClass_stats)


simp_logreg_byClass_stats_table <- simp_logreg_byClass_stats %>%
  rownames_to_column("Statistic") %>% # turn the statistics into a column
  kable(
    caption = "Simple Logistic Regression Statistics by Class",
    digits = 4,
    align = "lccccc",
    booktabs = TRUE
  ) %>%
  kable_styling(
    latex_options = c("striped", "hold_position", "scale_down"),
    font_size = 9
  )

simp_logreg_byClass_stats_table
```

```{r simple-logistic-regression-roc-curve, fig.cap="**Simple Logistic Regression ROC Plot:** *ROC plot for Class D vs Not Class D*"}
simple_logreg_probabilities <- predict(simple_logreg_model, test_data, type = "probs") # using type=probs gives a probability of the label value. This lets us know when the model is making confident or unconfident predictions
is_class_d <- ifelse(test_data$label =="D", 1, 0)
class_d_probs <- as.data.frame(simple_logreg_probabilities)$D

# these produce a 1 and 0 for true and false for the actual value of class D for each row in data, and then the probability of each row being class d according to the model

roc <- roc(is_class_d, class_d_probs)
auc_value <- auc(roc)

plot(roc, main = paste("ROC Curve: Class D vs. All (AUC =", round(auc_value, 3), ")"),
     col = "blue", lwd = 2, 
     xlab = "False Positive Rate (1 - Specificity)", 
     ylab = "True Positive Rate (Sensitivity)")


abline(a = 0, b = 1, lty = 2, col = "gray")
```

```{r logistic-regression-feature-selection}
train_data_logreg_fs1 <- train_data %>% select(X1, X2, X3, X4, X5, X6, X7, X8, X9, X10, X11, X12, X13, X14, X15, X16, X18, label)
test_data_logreg_fs1 <- test_data %>% select(X1, X2, X3, X4, X5, X6, X7, X8, X9, X10, X11, X12, X13, X14, X15, X16, X18, label)
logreg_featselection_1_model <- multinom(
    label ~ ., # use all cols
    data = train_data_logreg_fs1,
    trace = FALSE
  )
logreg_featselection_1_predictions <- predict(logreg_featselection_1_model, test_data_logreg_fs1, type = "class") # using type-class gives a prediction of the 
logreg_featselection_1_conf_matrix = confusionMatrix(logreg_featselection_1_predictions, test_data_logreg_fs1$label)

logreg_featselection_1_conf_df <- as.data.frame(logreg_featselection_1_conf_matrix$table)

logreg_featselection_1_conf_df <- logreg_featselection_1_conf_df %>% pivot_wider(names_from = Reference, values_from = Freq)

logreg_featselection_1_confm_table <- logreg_featselection_1_conf_df %>%
  kable(
    caption = "Simple Logistic Regression with selected features Confusion Matrix", 
    digits = 7, 
    align = "c",
    booktabs = TRUE,
    na="-"
  ) %>% 
  kable_styling(
    latex_options = c("striped", "hold_position", "scale_down"),
    font_size = 9
  ) %>%
  add_header_above(
    c(" " = 1, "Reference" = 4)
  ) #%>% 
  # pack_rows(
  #   "Prediction",
  #   1,
  #   4
  # )

logreg_featselection_1_confm_table


logreg_featselection_1_overall_stats <- data.frame(
  Statistic = names(logreg_featselection_1_conf_matrix$overall),
  Value = logreg_featselection_1_conf_matrix$overall
)


logreg_featselection_1_overall_stats_table <- logreg_featselection_1_overall_stats %>%
  kable(
    caption = "Simple Logistic Regression with selected features Overall Statistics",
    col.names = c("Statistic", "Value"),
    digits = 4,
    align = "lc",
    booktabs = TRUE
  ) %>%
  kable_styling(
    latex_options = c("striped", "hold_position"),
    font_size = 9
  )

logreg_featselection_1_overall_stats_table



logreg_featselection_1_byClass_stats <- as.data.frame(logreg_featselection_1_conf_matrix$byClass)
logreg_featselection_1_byClass_stats <- t(logreg_featselection_1_byClass_stats) # I want the class names to be columns not rows
logreg_featselection_1_byClass_stats <- as.data.frame(logreg_featselection_1_byClass_stats)


logreg_featselection_1_byClass_stats_table <- logreg_featselection_1_byClass_stats %>%
  rownames_to_column("Statistic") %>% # turn the statistics into a column
  kable(
    caption = "Simple Logistic Regression with selected features Statistics by Class",
    digits = 4,
    align = "lccccc",
    booktabs = TRUE
  ) %>%
  kable_styling(
    latex_options = c("striped", "hold_position", "scale_down"),
    font_size = 9
  )

logreg_featselection_1_byClass_stats_table






train_data_logreg_fs2 <- train_data %>% select(X1, X2, X3, X4, X5, X6, X7, X10, X11, X12, X13, X14, X15, X16, X17, X18, X19, X20, label)
test_data_logreg_fs2 <- test_data %>% select(X1, X2, X3, X4, X5, X6, X7, X10, X11, X12, X13, X14, X15, X16, X17, X18, X19, X20, label)
logreg_featselection_2_model <- multinom(
    label ~ ., # use all cols
    data = train_data_logreg_fs2,
    trace = FALSE
  )
logreg_featselection_2_predictions <- predict(logreg_featselection_2_model, test_data_logreg_fs2, type = "class") # using type-class gives a prediction of the 
logreg_featselection_2_conf_matrix = confusionMatrix(logreg_featselection_2_predictions, test_data_logreg_fs2$label)

logreg_featselection_2_conf_df <- as.data.frame(logreg_featselection_2_conf_matrix$table)

logreg_featselection_2_conf_df <- logreg_featselection_2_conf_df %>% pivot_wider(names_from = Reference, values_from = Freq)

logreg_featselection_2_confm_table <- logreg_featselection_2_conf_df %>%
  kable(
    caption = "Simple Logistic Regression with selected features Confusion Matrix", 
    digits = 7, 
    align = "c",
    booktabs = TRUE,
    na="-"
  ) %>% 
  kable_styling(
    latex_options = c("striped", "hold_position", "scale_down"),
    font_size = 9
  ) %>%
  add_header_above(
    c(" " = 1, "Reference" = 4)
  ) #%>% 
  # pack_rows(
  #   "Prediction",
  #   1,
  #   4
  # )

logreg_featselection_2_confm_table


logreg_featselection_2_overall_stats <- data.frame(
  Statistic = names(logreg_featselection_2_conf_matrix$overall),
  Value = logreg_featselection_2_conf_matrix$overall
)


logreg_featselection_2_overall_stats_table <- logreg_featselection_2_overall_stats %>%
  kable(
    caption = "Simple Logistic Regression with selected features Overall Statistics",
    col.names = c("Statistic", "Value"),
    digits = 4,
    align = "lc",
    booktabs = TRUE
  ) %>%
  kable_styling(
    latex_options = c("striped", "hold_position"),
    font_size = 9
  )

logreg_featselection_2_overall_stats_table



logreg_featselection_2_byClass_stats <- as.data.frame(logreg_featselection_2_conf_matrix$byClass)
logreg_featselection_2_byClass_stats <- t(logreg_featselection_2_byClass_stats) # I want the class names to be columns not rows
logreg_featselection_2_byClass_stats <- as.data.frame(logreg_featselection_2_byClass_stats)


logreg_featselection_2_byClass_stats_table <- logreg_featselection_2_byClass_stats %>%
  rownames_to_column("Statistic") %>% # turn the statistics into a column
  kable(
    caption = "Simple Logistic Regression with selected features Statistics by Class",
    digits = 4,
    align = "lccccc",
    booktabs = TRUE
  ) %>%
  kable_styling(
    latex_options = c("striped", "hold_position", "scale_down"),
    font_size = 9
  )

logreg_featselection_2_byClass_stats_table
```



```{r weighted-logistic-regression}
class_weights <- train_data %>%
  count(label) %>%
  mutate(weight = 1/n, # n just always gets the value for number of observations
         weight = weight/sum(weight) * n()) # regularize using the mean

weights_vector <- train_data %>%
  left_join(class_weights, by = "label") %>%
  pull(weight)  # gets just the weight column as a vector

weighted_model <- multinom(label ~ ., data = train_data, 
                          weights = weights_vector,
                          trace = FALSE)

weighted_pred <- predict(weighted_model, test_data)
weighted_logreg_conf_matrix <- confusionMatrix(weighted_pred, test_data$label)

weighted_logreg_conf_df <- as.data.frame(weighted_logreg_conf_matrix$table)

weighted_logreg_conf_df <- weighted_logreg_conf_df %>% pivot_wider(names_from = Reference, values_from = Freq)

weighted_logreg_confm_table <- weighted_logreg_conf_df %>%
  kable(
    caption = "Weighted Logistic Regression Confusion Matrix", 
    digits = 7, 
    align = "c",
    booktabs = TRUE,
    na="-"
  ) %>% 
  kable_styling(
    latex_options = c("striped", "hold_position", "scale_down"),
    font_size = 9
  ) %>%
  add_header_above(
    c(" " = 1, "Reference" = 4)
  ) #%>% 
  # pack_rows(
  #   "Prediction",
  #   1,
  #   4
  # )

weighted_logreg_confm_table


weighted_logreg_overall_stats <- data.frame(
  Statistic = names(weighted_logreg_conf_matrix$overall),
  Value = simple_logreg_conf_matrix$overall
)


weighted_logreg_overall_stats_table <- weighted_logreg_overall_stats %>%
  kable(
    caption = "Weighted Logistic Regression Overall Statistics",
    col.names = c("Statistic", "Value"),
    digits = 4,
    align = "lc",
    booktabs = TRUE
  ) %>%
  kable_styling(
    latex_options = c("striped", "hold_position"),
    font_size = 9
  )

weighted_logreg_overall_stats_table



weighted_logreg_byClass_stats <- as.data.frame(weighted_logreg_conf_matrix$byClass)
weighted_logreg_byClass_stats <- t(weighted_logreg_byClass_stats) # I want the class names to be columns not rows
weighted_logreg_byClass_stats <- as.data.frame(weighted_logreg_byClass_stats)


weighted_logreg_byClass_stats_table <- weighted_logreg_byClass_stats %>%
  rownames_to_column("Statistic") %>% # turn the statistics into a column
  kable(
    caption = "Weighted Logistic Regression Statistics by Class",
    digits = 4,
    align = "lccccc",
    booktabs = TRUE
  ) %>%
  kable_styling(
    latex_options = c("striped", "hold_position", "scale_down"),
    font_size = 9
  )

weighted_logreg_byClass_stats_table

```

```{r regularized-logistic-regression-summary-table}
decay_values <- c(0.001, 0.01, 0.1, 0.5, 1, 2, 10) # these are the regularization parameters we are going to try

# creating a df to store the results of all models - will turn this into the report table
results_df <- data.frame(
  Decay = as.character(decay_values),
  Accuracy = numeric(length(decay_values)),
  Kappa = numeric(length(decay_values)),
  F1_Class_A = numeric(length(decay_values)),
  F1_Class_B = numeric(length(decay_values)),
  F1_Class_C = numeric(length(decay_values)),
  F1_Class_D = numeric(length(decay_values))
)

for (i in 1:length(decay_values)) {
  decay <- decay_values[i]
  model <- multinom(label ~ ., data = train_data, 
                   decay = decay, 
                   trace = FALSE)
  predictions <- predict(model, test_data)
  conf_matrix <- confusionMatrix(predictions, test_data$label)
  
  results_df$Accuracy[i] <- conf_matrix$overall["Accuracy"]
  results_df$Kappa[i] <- conf_matrix$overall["Kappa"]
  
  class_metrics <- conf_matrix$byClass
  class_names <- rownames(class_metrics)
  
  for (j in 1:length(class_names)) {
    class_name <- class_names[j]
    class_letter <- substr(class_name, nchar(class_name), nchar(class_name))
    col_name <- paste0("F1_Class_", class_letter)
    results_df[i, col_name] <- class_metrics[j, "F1"]
  }
  
  if (i == 1 || conf_matrix$overall["Accuracy"] > best_accuracy) {
    best_accuracy <- conf_matrix$overall["Accuracy"]
    best_decay <- decay
    best_conf_matrix <- conf_matrix
    best_predictions <- predictions
  }
}


decay_summary_table <- results_df %>%
  kable(
    caption = "Logistic Regression Performance with Different Regularization Parameters",
    col.names = c("Decay", "Accuracy", "Kappa", 
                  "F1 Score (A)", "F1 Score (B)", "F1 Score (C)", "F1 Score (D)"),
    digits = 4,
    align = "lcccccc",
    booktabs = TRUE
  ) %>%
  kable_styling(
    latex_options = c("striped", "hold_position", "scale_down"),
    font_size = 9
  )
decay_summary_table
```

```{r regularized-logistic-regression-viz, fig.cap="**Regularized Logistic Regression**"}

ggplot(results_df %>% mutate(Decay = as.numeric(Decay)) %>% pivot_longer(cols = c(Accuracy, F1_Class_A, F1_Class_B, F1_Class_C, F1_Class_D),
                                  names_to = "Metric", values_to = "Value")) +
  geom_line(aes(x = Decay, y = Value, color = Metric)) +
  geom_point(aes(x = Decay, y = Value, color = Metric)) +
  scale_x_log10() +  # Log scale for better visualization of decay range
  labs(title = "Effect of Regularization on Logistic Regression Performance",
       x = "Decay Parameter (log scale)",
       y = "Performance Metric Value") +
  theme_minimal()
```
```{r regularized-logistic-regression-conf-matrix}

best_reg_conf_df <- as.data.frame(best_conf_matrix$table)
best_reg_conf_df <- best_reg_conf_df %>% pivot_wider(names_from = Reference, values_from = Freq)
best_reg_conf_table <- best_reg_conf_df %>%
  kable(
    caption = paste("Confusion Matrix for Best Regularized Logistic Regression Model (Decay =", best_decay, ")"),
    digits = 0,
    align = "c",
    booktabs = TRUE
  ) %>% 
  kable_styling(
    latex_options = c("striped", "hold_position"),
    font_size = 9
  ) %>%
  add_header_above(
    c(" " = 1, "Reference" = 4)
  )
best_reg_conf_table

```

logistic regression was not great. weighting did nothing, as expected. regularization didn't really do anything. feature selection did not improve the model. We saw that the model particularly underperformed at classifying class D correctly.

### Random Forest

```{r random-forest-base}
rf_model <- randomForest(
  label ~ ., 
  data = train_data,
  ntree = 500,           # number of trees we will use to start - can tune this later
  mtry = floor(sqrt(20)), # a typical default is sqrt(num of features) features per tree in the forest. can tune this later
  importance = TRUE       
)
rf_predictions <- predict(rf_model, test_data)
rf_conf_matrix <- confusionMatrix(rf_predictions, test_data$label)

rf_conf_df <- as.data.frame(rf_conf_matrix$table)
rf_conf_df <- rf_conf_df %>% pivot_wider(names_from = Reference, values_from = Freq)

rf_confm_table <- rf_conf_df %>%
  kable(
    caption = "Basic Random Forest Confusion Matrix", 
    digits = 7, 
    align = "c",
    booktabs = TRUE,
    na="-"
  ) %>% 
  kable_styling(
    latex_options = c("striped", "hold_position", "scale_down"),
    font_size = 9
  ) %>%
  add_header_above(
    c(" " = 1, "Reference" = 4)
  ) #%>% 
  # pack_rows(
  #   "Prediction",
  #   1,
  #   4
  # )

rf_confm_table


rf_overall_stats <- data.frame(
  Statistic = names(rf_conf_matrix$overall),
  Value = rf_conf_matrix$overall
)


rf_overall_stats_table <- rf_overall_stats %>%
  kable(
    caption = "Basic Random Forest Overall Statistics",
    col.names = c("Statistic", "Value"),
    digits = 4,
    align = "lc",
    booktabs = TRUE
  ) %>%
  kable_styling(
    latex_options = c("striped", "hold_position"),
    font_size = 9
  )

rf_overall_stats_table



rf_byClass_stats <- as.data.frame(rf_conf_matrix$byClass)
rf_byClass_stats <- t(rf_byClass_stats) # I want the class names to be columns not rows
rf_byClass_stats <- as.data.frame(rf_byClass_stats)


rf_byClass_stats_table <- rf_byClass_stats %>%
  rownames_to_column("Statistic") %>% # turn the statistics into a column
  kable(
    caption = "Basic Random Forest Statistics by Class",
    digits = 4,
    align = "lccccc",
    booktabs = TRUE
  ) %>%
  kable_styling(
    latex_options = c("striped", "hold_position", "scale_down"),
    font_size = 9
  )

rf_byClass_stats_table
```

```{r random-forest-base-feature-importance-table}

importance_values <- as.data.frame(importance(rf_model) ) %>% 
  arrange(desc(MeanDecreaseAccuracy))
feature_importance_table <- importance_values %>%
  kable(
    caption = "Basic Random Forest Feature Importance", 
    digits = 3, 
    align = "c",
    booktabs = TRUE,
    na="-"
  ) %>% 
  kable_styling(
    latex_options = c("striped", "hold_position", "scale_down"),
    font_size = 9
  )

feature_importance_table
```


```{r random-forest-remove-unimportant-features}

selected_train_data_1 <- train_data %>%
  select(-X17, -X19, -X20, -X15, -X6)
selected_test_data_1 <- test_data %>%
  select(-X17, -X19, -X20, -X15, -X6)
rf_feat_selection_1_model <- randomForest(
  label ~ ., 
  data = selected_train_data_1,
  ntree = 500,           # number of trees we will use to start - can tune this later
  mtry = floor(sqrt(20)), # a typical default is sqrt(num of features) features per tree in the forest. can tune this later
  importance = TRUE       
)

rf_fs_1_predictions <- predict(rf_feat_selection_1_model, selected_test_data_1)

rf_fs_1_conf_matrix <- confusionMatrix(rf_fs_1_predictions, selected_test_data_1$label)


rf_fs_1_conf_df <- as.data.frame(rf_fs_1_conf_matrix$table)
rf_fs_1_conf_df <- rf_fs_1_conf_df %>% pivot_wider(names_from = Reference, values_from = Freq)

rf_fs_1_confm_table <- rf_fs_1_conf_df %>%
  kable(
    caption = "Random Forest (5 least important features removed) Confusion Matrix", 
    digits = 7, 
    align = "c",
    booktabs = TRUE,
    na="-"
  ) %>% 
  kable_styling(
    latex_options = c("striped", "hold_position", "scale_down"),
    font_size = 9
  ) %>%
  add_header_above(
    c(" " = 1, "Reference" = 4)
  ) #%>% 
  # pack_rows(
  #   "Prediction",
  #   1,
  #   4
  # )

rf_fs_1_confm_table


rf_fs_1_overall_stats <- data.frame(
  Statistic = names(rf_fs_1_conf_matrix$overall),
  Value = rf_fs_1_conf_matrix$overall
)


rf_fs_1_overall_stats_table <- rf_fs_1_overall_stats %>%
  kable(
    caption = "Random Forest (5 least important features removed) Overall Statistics",
    col.names = c("Statistic", "Value"),
    digits = 4,
    align = "lc",
    booktabs = TRUE
  ) %>%
  kable_styling(
    latex_options = c("striped", "hold_position"),
    font_size = 9
  )

rf_fs_1_overall_stats_table



rf_fs_1_byClass_stats <- as.data.frame(rf_fs_1_conf_matrix$byClass)
rf_fs_1_byClass_stats <- t(rf_fs_1_byClass_stats) # I want the class names to be columns not rows
rf_fs_1_byClass_stats <- as.data.frame(rf_fs_1_byClass_stats)


rf_fs_1_byClass_stats_table <- rf_fs_1_byClass_stats %>%
  rownames_to_column("Statistic") %>% # turn the statistics into a column
  kable(
    caption = "Random Forest (5 least important features removed) Statistics by Class",
    digits = 4,
    align = "lccccc",
    booktabs = TRUE
  ) %>%
  kable_styling(
    latex_options = c("striped", "hold_position", "scale_down"),
    font_size = 9
  )

rf_fs_1_byClass_stats_table
```

```{r random-forest-remove-unimportant-features-feature-importance}

rf_fs_1_importance_values <- as.data.frame(importance(rf_feat_selection_1_model) ) %>% 
  arrange(desc(MeanDecreaseAccuracy))
rf_fs_1_feature_importance_table <- rf_fs_1_importance_values %>%
  kable(
    caption = "Random Forest (5 least important features removed) Feature Importance", 
    digits = 3, 
    align = "c",
    booktabs = TRUE,
    na="-"
  ) %>% 
  kable_styling(
    latex_options = c("striped", "hold_position", "scale_down"),
    font_size = 9
  )

rf_fs_1_feature_importance_table
```

```{r random-forest-remove-important-features}

selected_train_data_2 <- train_data %>%
  select(-X8)
selected_test_data_2 <- test_data %>%
  select(-X8)
rf_feat_selection_2_model <- randomForest(
  label ~ ., 
  data = selected_train_data_2,
  ntree = 500,           # number of trees we will use to start - can tune this later
  mtry = floor(sqrt(20)), # a typical default is sqrt(num of features) features per tree in the forest. can tune this later
  importance = TRUE       
)

rf_fs_2_predictions <- predict(rf_feat_selection_2_model, selected_test_data_2)

rf_fs_2_conf_matrix <- confusionMatrix(rf_fs_2_predictions, selected_test_data_2$label)


rf_fs_2_conf_df <- as.data.frame(rf_fs_2_conf_matrix$table)
rf_fs_2_conf_df <- rf_fs_2_conf_df %>% pivot_wider(names_from = Reference, values_from = Freq)

rf_fs_2_confm_table <- rf_fs_2_conf_df %>%
  kable(
    caption = "Random Forest (5 least important features removed) Confusion Matrix", 
    digits = 7, 
    align = "c",
    booktabs = TRUE,
    na="-"
  ) %>% 
  kable_styling(
    latex_options = c("striped", "hold_position", "scale_down"),
    font_size = 9
  ) %>%
  add_header_above(
    c(" " = 1, "Reference" = 4)
  ) #%>% 
  # pack_rows(
  #   "Prediction",
  #   1,
  #   4
  # )

rf_fs_2_confm_table


rf_fs_2_overall_stats <- data.frame(
  Statistic = names(rf_fs_2_conf_matrix$overall),
  Value = rf_fs_2_conf_matrix$overall
)


rf_fs_2_overall_stats_table <- rf_fs_2_overall_stats %>%
  kable(
    caption = "Random Forest (5 least important features removed) Overall Statistics",
    col.names = c("Statistic", "Value"),
    digits = 4,
    align = "lc",
    booktabs = TRUE
  ) %>%
  kable_styling(
    latex_options = c("striped", "hold_position"),
    font_size = 9
  )

rf_fs_2_overall_stats_table



rf_fs_2_byClass_stats <- as.data.frame(rf_fs_2_conf_matrix$byClass)
rf_fs_2_byClass_stats <- t(rf_fs_2_byClass_stats) # I want the class names to be columns not rows
rf_fs_2_byClass_stats <- as.data.frame(rf_fs_2_byClass_stats)


rf_fs_2_byClass_stats_table <- rf_fs_2_byClass_stats %>%
  rownames_to_column("Statistic") %>% # turn the statistics into a column
  kable(
    caption = "Random Forest (5 least important features removed) Statistics by Class",
    digits = 4,
    align = "lccccc",
    booktabs = TRUE
  ) %>%
  kable_styling(
    latex_options = c("striped", "hold_position", "scale_down"),
    font_size = 9
  )

rf_fs_2_byClass_stats_table
```
```{r random-forest-remove-important-features-feature-importance}

rf_fs_2_importance_values <- as.data.frame(importance(rf_feat_selection_2_model) ) %>% 
  arrange(desc(MeanDecreaseAccuracy))
rf_fs_2_feature_importance_table <- rf_fs_2_importance_values %>%
  kable(
    caption = "Random Forest (5 least important features removed) Feature Importance", 
    digits = 3, 
    align = "c",
    booktabs = TRUE,
    na="-"
  ) %>% 
  kable_styling(
    latex_options = c("striped", "hold_position", "scale_down"),
    font_size = 9
  )

rf_fs_2_feature_importance_table
```

```{r tuning-random-forest, fig.cap="**Optimal mtry for Random Forest**"}

train_data_no_label <- train_data %>% select(- label)

tuneRF_result <- tuneRF(
  x = train_data_no_label,
  y = train_data$label,
  ntreeTry = 500,
  mtryStart = 4,         # = sqrt(num of features) as before
  stepFactor = 1.5,      # this func trys a step above and below the starting value of mtry by multiplying by this number. 1.5 is the smallest step we can be sure will round to a number bigger/smaller that our stat
  improve = 0.01,        # model only has to imrove slightly for tuning to continue
  trace = FALSE, # turn off trace for report          
  plot = TRUE            
)


optimal_mtry <- tuneRF_result[which.min(tuneRF_result[, "OOBError"]), "mtry"]

```
```{r}
rf_tuned_model <- randomForest(
  label ~ ., 
  data = train_data,
  ntree = 500,           # number of trees we will use to start - can tune this later
  mtry = 9, # optimal value from tuning with RFTune
  importance = TRUE       
)


rf_tuned_predictions <- predict(rf_tuned_model, test_data)
rf_tuned_conf_matrix <- confusionMatrix(rf_tuned_predictions, test_data$label)

rf_tuned_conf_df <- as.data.frame(rf_tuned_conf_matrix$table)
rf_tuned_conf_df <- rf_tuned_conf_df %>% pivot_wider(names_from = Reference, values_from = Freq)

rf_tuned_confm_table <- rf_tuned_conf_df %>%
  kable(
    caption = "Tuned Random Forest Confusion Matrix", 
    digits = 7, 
    align = "c",
    booktabs = TRUE,
    na="-"
  ) %>% 
  kable_styling(
    latex_options = c("striped", "hold_position", "scale_down"),
    font_size = 9
  ) %>%
  add_header_above(
    c(" " = 1, "Reference" = 4)
  ) #%>% 
  # pack_rows(
  #   "Prediction",
  #   1,
  #   4
  # )

rf_tuned_confm_table


rf_tuned_overall_stats <- data.frame(
  Statistic = names(rf_conf_matrix$overall),
  Value = rf_conf_matrix$overall
)


rf_tuned_overall_stats_table <- rf_tuned_overall_stats %>%
  kable(
    caption = "Tuned Random Forest Overall Statistics",
    col.names = c("Statistic", "Value"),
    digits = 4,
    align = "lc",
    booktabs = TRUE
  ) %>%
  kable_styling(
    latex_options = c("striped", "hold_position"),
    font_size = 9
  )

rf_tuned_overall_stats_table



rf_tuned_byClass_stats <- as.data.frame(rf_tuned_conf_matrix$byClass)
rf_tuned_byClass_stats <- t(rf_tuned_byClass_stats) # I want the class names to be columns not rows
rf_tuned_byClass_stats <- as.data.frame(rf_tuned_byClass_stats)


rf_tuned_byClass_stats_table <- rf_tuned_byClass_stats %>%
  rownames_to_column("Statistic") %>% # turn the statistics into a column
  kable(
    caption = "Tuned Random Forest Statistics by Class",
    digits = 4,
    align = "lccccc",
    booktabs = TRUE
  ) %>%
  kable_styling(
    latex_options = c("striped", "hold_position", "scale_down"),
    font_size = 9
  )

rf_tuned_byClass_stats_table

```




The random forest performed well without any configuration. feature selection was not effective. still struggled at seperating class D. mtry was tuned. 

### SVM

svm was good but not as good as random forest. lots of tuning. feature selection was uneffective


## Unsupervised Learning Results

### Agglomerative Hierarchical Clustering

ahc was good not great.

### Gaussian Mixed Model Clustering

Gaussian mixed model clustering performed very well. dimensionality reduction using factor analysis was slightly effective. 

# Discussion

the final model had good overall accuracy but caution is advised when using a ml model with this data due to the poor performance in class D - if false positives or false negatives in this class have serious implications, some models become immediately unusable. 



# Conclusion



# References


