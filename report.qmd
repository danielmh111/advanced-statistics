---
title: "Advanced Statistics: Application of supervised and unsupervised methods to biological data"
author: "Daniel Hill"
date: "27 April 2025"
format: 
  pdf:
    documentclass: scrartcl
    classoption: twocolumn
    number-sections: true
    toc: true
    toc-depth: 3
    toc-title: Contents
    lof: true
    lot: true
    geometry:
      - paper=a4paper    
      - top=15mm         
      - bottom=15mm      
      - left=15mm        
      - right=15mm       
      - columnsep=7mm    
      - heightrounded
    dev: pdf
    df-print: kable
    natbiboptions: "authoryear,round"
    include-in-header:
      text: |
        \usepackage{etoolbox}
        \pretocmd{\tableofcontents}{\newpage}{}{}
        \usepackage{caption} 
abstract: "**Abstract:** Biological data with twenty features and four categorical class labels is explored and analysed using advanced statistical techniques in this report. Both supervised and unsupervised methods were implemented and evaluated, and the broad selection of models includes logistic regression, support vector machine, random forest, agglomerative hierarchical clustering, and gaussian mixture modelling. Comparing the results achieved using a selection of models with different underlying principles gives insight into the nature of the data. For example, the success of model-based clustering compared to hierarchical clustering and tree-based learning suggests the lack of hierarchy among the categorical labels, and the success of factor analysis as a dimensionality reduction technique suggests the presence of underlying biological mechanisms leading to several of the features arising together. Models achieving over 90% accuracy were produced, but all models performed notably worse at separating one of the categories that overlapped the other three."
bibliography: references.bib
editor: visual
include-in-header:
  text: |
    \usepackage{ltablex}
    \usepackage{titling}
    \pretitle{\begin{center}\LARGE\bfseries}
    \posttitle{\end{center}}
    \preauthor{\begin{center}\large}
    \postauthor{\end{center}}
    \predate{\begin{center}}
    \postdate{\end{center}}
---

\newpage

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,        
  warning = FALSE,     
  message = FALSE,
  fig.align = "center",
  fig.width = 6,
  fig.height = 4,
  out.width = "80%"
)
```

```{r load-libraries}
library(tidyverse)
library(carat)
library(caret)
library(skimr)
library(kableExtra)
library(reshape2)
library(viridis)
library(hopkins)
library(mnet)
library(nnet)
library(pROC)
library(randomForest)
library(e1071) 
library(cluster)   
library(factoextra)   
library(dendextend) 
library(circlize)
library(fpc)            
library(NbClust)  
library(mclust)  
library(GGally)  
library(ggpubr) 
library(psych)
```
```{r table-format-func}
full_width_table <- function(kbl_output) {
  cat("\\begin{table*}[ht]\n")
  cat("\\centering\n")
  kbl_output
  cat("\\end{table*}\n")
}
```

# Introduction

This report takes a moderately sized biological dataset containing 3000 observations, where each observation having 20 numeric varables and one catagorical label. These data are explored thoroughly before being used to train and test an array of statistical modelling techniques.

This is an interesting project because the origin and meaning of the variables in the data are completely unknown - an unusual scenario in the data science field, where usually it is the domain knowledge and problem context that inform the selection and implementation of statistical methods. Here, with this relationship reversed, algorithms have been chosen so that the evaluation of their performance can attempt to uncoverthe underlying biological significance of the variables.

Achieving meaningful results in this task shows the importance of supervised and unsupervised learning to this field, where classification algorithms can build valuable models that have high impact on society such as disease diagnosis models, and unsupervised learning techniques can create breakthoughs in identifying clusters of data that lead to new discoveries and classifications.

# Methods

## Data Description

Each of the 3000 observations has 20 numeric features and a label placing it in one of four catagorical classes. Though exploritory data analysis, two groups of correlated features were identified. Outliers were identified and removed using z score method. One feature transformed using logarithm to create a more normal distribution to improve the performance of models. All features were scaled and centered. After the preprocessing of the data, 2776 usable observations remained.

Bootstrap sampling was used to create a larger dataset so that the performance of the models could be compared between the original and bootstrapped data.

## Exploratory Data Analysis Approach

find distributions within each feature, look for correlations between features, scatter between plots that features that have high correlation to the catagorical label or another feature, use PCA to visualize all data together, calculate hopkins statistic to determine the clustering tendency of the data

\onecolumn

```{r feature-data-descriptions}
#| cache: true

data <- read_csv("Data(2).csv")

desc <- skim(data)
desc <- desc %>% 
  select(
    skim_variable, 
    n_missing, 
    numeric.mean, 
    numeric.sd, 
    numeric.p0, 
    numeric.p25, 
    numeric.p50, 
    numeric.p75, 
    numeric.p100, 
  ) %>% 
  filter(
    skim_variable != "label"
  ) %>%
  rename(
    "Variable Name" = skim_variable, 
    "No. missing values" = n_missing, 
    "mean" = numeric.mean, 
    "Std deviation" = numeric.sd, 
    "min" = numeric.p0, 
    "25th %ile" = numeric.p25, 
    "median" = numeric.p50, 
    "75th %ile" = numeric.p75, 
    "max" = numeric.p100, 
  ) %>%
  mutate(across(everything(), ~ifelse(. == "NA", "-", .))) 

desc_table <- desc %>% 
  kable(
    caption = "Feature Descriptions", 
    digits = 3, 
    align = "c",
    booktabs = TRUE,
    na="-",
    longtable = T
  ) %>% 
  kable_styling(
    latex_options = c("striped", "hold_position", "scale_down"),
    font_size = 9,
    full_width = FALSE
  )

desc_table
```

\twocolumn


This description of the predictive features shows the range of scales, ranging by and order of magnitude. Several models such as support vector machine analysis are affected by the scale and centering of the data it learns from, so it this was identified as an important preprocessing step that had to be performed.

```{r correlation-heatmap, fig.cap="**Feature and Class Correlation Matrix:** *highlighting relationships between variables and relationships with catagorical labels*"}
#| cache: true

one_hot_data <- model.matrix(~ label - 1, data = data)
one_hot_data <- data %>%
  select(-label) %>%
  bind_cols(one_hot_data)

cor_matrix <- cor(one_hot_data, use = "complete.obs", method = "pearson")
cor_data <- melt(cor_matrix)

ggplot(cor_data, aes(x = Var1, y = Var2, fill = value)) +
  geom_tile(
    color = "white", 
    size = 0.25
  ) +
  scale_fill_gradient2(
    low = "darkblue",
    high = "darkred",
    mid = "white",
    name = "Correlation",
    limit = c(-1, 1), 
    guide = guide_colorbar(barwidth = 1, barheight = 10)
  ) + 
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 60, vjust = 0.75, hjust = 0.5, size = 7),  # all the feature names overlap if horisontal
    axis.text.y = element_text(size = 7),
    axis.title = element_blank(),           # axis titles arent meaningful
    panel.grid = element_blank(),       
    aspect.ratio = 1,                       # make plot square
    legend.position = "right",
    plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
    plot.subtitle = element_text(hjust = 0.5, size = 10)
  ) +
  coord_fixed()
```

Within the data there are two groups of features that correlate together - X7, X8, and X9, and X17 to X20. Noticing groups of correlated features is important since some models such as logistic regression and SVM will struggle with multicolinearity. This will lead us to attempt feature selection or dimensionality reduction with these models, or choose alternative algorithms that are more robust in these cases such as tree-based algorithms.

The difference between these two group is that while X7, X8, and X9 are three features with some of the strongest correlations with the label values, all of X17 to X20 are features without significant correlations. This would lead us to believe that X17 - X20 have low predictive power in classification that aim to predict the class label and so removing them entirely would be a justifyable approach.

Among the other columns, we see that there are definitly some columns with stronger correlations than others.

To produce the correlation matrix, the four catagorical labels were one hot encoded to create four binary columns. This allows us to see that several features have strong predictive power for one or more label but not all. For example, X8 has high correlations with classes A, B, and C, but none very low correlation with class D. This contrasts with a feature like X11 which has equal magnitude of correlation across all four labels.

```{r column-histograms, fig.cap="**Distributions within feature columns:** *histograms showing scale and skewness of data*"}
#| cache: true

data_long <- drop_na(data) %>%
  select(- starts_with("label")) %>%
  pivot_longer(
    cols = everything(), 
    names_to = "variable", 
    values_to = "value"
  ) %>%
  mutate(
    dist_group = case_when(
      variable == "X7" ~ "Bimodal",
      variable == "X8" ~ "Skewed",
      TRUE ~ "Normal"
    )
  )

ggplot(data_long, aes(x = value, fill = dist_group)) +
  geom_histogram(
    bins = 50, 
    alpha = 0.9,
    size = 0.2
  ) +
  scale_fill_manual(
    values = c("Normal" = "skyblue", "Bimodal" = "pink", "Skewed" = "purple"),
    name = "Distribution"
  ) +
  facet_wrap(
    ~ variable, 
    scales = "free"
    ) +
  theme_minimal() +
  labs(
    x = "Value", 
    y = "Count", 
    title = "Histograms of All Columns"
  )

```

The majority of the twenty predictive variables appeared to follow a normal distribution. Noteable exceptions were X8, which is heavily right skewed, and X7 which has a bimodal appearance. With both of these features showing strong correlations with the labels leading to a high probability that they have strong predictive power, they should not be removed. X8 will be transformed, and the natural logarithm of X8 will be used in all modelling.

```{r class-boxplots, fig.cap="**Distributions within feature columns:** *Boxplots by class label by feature*"}
#| cache: true

data_clean <- data %>% drop_na()
clean_and_long <- pivot_longer(data_clean, cols = 1:20, names_to= "Feature", values_to = "Value")
boxplot_data <- clean_and_long %>%
  mutate(
    important = case_when(
      Feature %in% c("X7", "X8", "X9", "X11", "X3", "X2") ~ "1",
      Feature %in% c("X20", "X19", "X18", "X17") ~ "-1",
      TRUE ~ "0"
    )
  )

boxplot <- ggplot( boxplot_data,  aes(label, Value, colour=label)  ) +
  geom_boxplot(
    outlier.shape = 21,
    outlier.size = 1,
    outlier.alpha = 0.5,
    width = 0.7
  ) +
  facet_wrap(~Feature, scales="free_y") +
  theme_minimal() +
  theme(
    strip.text = element_text(face = "bold", size = 10),
    panel.spacing = unit(0.5, "lines"),
    legend.position = "right",
    axis.text.x = element_text(angle = 0, size = 8),
    axis.text.y = element_blank(),
    axis.title.y = element_blank(),
    axis.title.x = element_blank(),
    panel.grid.minor = element_blank(),
    panel.grid.major.x = element_blank(),
    # panel.border = element_rect(
    #   colour = case_when(
    #     boxplot_data$important == "1" ~ "red",
    #     boxplot_data$important == "-1" ~ "lightblue",
    #     boxplot_data$important == "0" ~ "white"
    #   ),
    #   fill = NA,
    #   size = 0.5
    # )
  ) +
  geom_rect(data = subset(boxplot_data, important == "1") %>% 
              distinct(Feature, important),
            aes(xmin = -Inf, xmax = Inf, ymin = -Inf, ymax = Inf),
            color = "red", fill = NA, size = 0.5, alpha = 0.8,
            inherit.aes = FALSE) +
  geom_rect(data = subset(boxplot_data, important == "-1") %>% 
              distinct(Feature, important),
            aes(xmin = -Inf, xmax = Inf, ymin = -Inf, ymax = Inf),
            color = "blue", fill = NA, size = 0.5, alpha = 0.8,
            inherit.aes = FALSE)

boxplot
```

Across the entire dataset, the distribution of labels is uniform, with roughly on quarter of the observations falling into each category.

When the distribution of each feature is observed by class label, we see that some features have significantly different characteristics for each class. In the figure, highlighted with a red boarder are features where we can see notable differences in the key descriptive statistics such as medians and interquartile ranges between different classes. Highlighted in blue boarders are the features where the boxplots look almost identical from one feature to another. This gives us more insight into which features will be important for building effective models.

```{r pca-plot, fig.cap="**PCA plotting of class labels:** *scatterplot showing clustering tendency of catagorical classes*"}
#| cache: true

data_clean <- data %>% drop_na() # pca wont work with null values in the frame
pc <- prcomp(data_clean[1:20],
            center = TRUE,
            scale = TRUE)
pc_data <- data.frame(pc$x)

labelled_pca <- bind_cols(pc_data[1:2], data.frame(data_clean$label), .name_repair = "universal")

ggplot(labelled_pca, aes(x=PC1, y=PC2, colour=data_clean.label)) +
      geom_point(
        alpha=0.4
        ,size=1
      ) +
      labs(
          colour="Class Label"
        ) +
  geom_density2d(alpha=0.75)
    
```

It is important that we learn about the structure of the underlying data in order to understand our chances of sucess with unsupervised methods where we do not have the value of the class label to train the model. Using Principle Componant Analysis, we can capture most of the information in the system in one scatter plot. This shows us that there is definitly some underlying structure to the data that will lead to the formation of clusters, although it is fairly loose in this plot. Annotating the points with the target label shows us that classes A, B, and C form elipsoidal groups that are roughly equal in size and orientation, and offset along the second principle component. The fourth class (D) forms a larger elipsoid that overlaps significantly with the other three.

The seperation of the first three classes suggests that there is information in the features that allow the statistical models to decern between them, but the overlap of class D means that this class may have more misclassifications. It may be challenging to find an approach that is effective for this label.

The fact that the three classes that are distinct are displaced along the second principle componant and not the first principle component means that it is the dimensions with less variation that distinguish them. It would be easier to seperate the groups with predictive models if they were offset along the first principle conmponent.

To learn more about the underlying structure of the dataset, the clustering tendency can be examined by calculating the Hopkins statistic, where a score close to 1 indicates strong clustering tenency, a score of 0.5 indicates random distribution of occurences, and 0 a uniform distribution.

```{r hopkins}
#| cache: true

data_clean <- read_csv("data_clean.csv") %>% mutate(label = as.factor(label))

hopkins_stat <- hopkins(data_clean[0:20], m = nrow(data_clean)/10) 
# we leave one row out to be the 'reference point' that we measure the distance to the other points from. i think. 

data_unlabelled <- data_clean %>% select(-label)
hopkins_stat_unlabelled <- hopkins(data_unlabelled[0:19], m = nrow(data_unlabelled)/10)

binary_data <- data_clean %>% mutate(label = factor( ifelse(label == "D", 1, 0), levels = c(0, 1) ))
binary_hopkins_stat <- hopkins(binary_data[0:20], m = nrow(binary_data)/10)

correlated_data <- data_clean %>%
  select(
    X2,
    X3,
    X7,
    X8,
    X9,
    X11
  )
hopkins_stat_high_diffs <- hopkins(correlated_data, m = nrow(correlated_data)/10) 


least_correlated_data <- data_clean %>%
  select(
    X17,
    X18,
    X19,
    X20
  )
hopkins_stat_low_corr <- hopkins(least_correlated_data, m = nrow(least_correlated_data)/10) 

boring_data <- data_clean %>%
  select(
    X1,
    X4,
    X6,
    X10,
    X12,
    X13,
    X14,
    X15,
    X16
  )
hopkins_stat_boring <- hopkins(boring_data, m = nrow(boring_data)/10)

# Print result
# print(hopkins_stat)
# print(hopkins_stat_unlabelled)
# print(binary_hopkins_stat)
# print(hopkins_stat_high_diffs)
# print(hopkins_stat_low_corr)
# print(hopkins_stat_boring)

names = c("All Features + Label", "All Features with Label Removed", "All Features Binary Class D vs Rest", "Features X2,X3,X7,X8,X9,X11", "Features X17,X18,X19,X20", "Features X1,X4,X6,X10,X12,X13,X14,X15,X16")
# print(names)
scores = c(hopkins_stat, hopkins_stat_unlabelled, binary_hopkins_stat, hopkins_stat_high_diffs, hopkins_stat_low_corr, hopkins_stat_boring)
# print(scores)
df <- data.frame(names, scores) %>% 
  rename(
    "Columns used" = names,
    "Hopkins Statistic Score" = scores
  )

hopkins_table <- df %>%
  kable(
    caption = "Hopkins Statistic Scores", 
    digits = 7, 
    align = "lc", # left align the column names, but centre the scores
    booktabs = TRUE,
    na="-",
    longtable = FALSE,
    format = "latex"
  ) %>% 
  kable_styling(
    latex_options = c("striped", "hold_position", "scale_down"),
    font_size = 9
  )

hopkins_table
```

We can see there is a very high clustering tendency for various treatments of the data. The statistic remains high when the label is removed from the feature set. This is promising for persuing unsupervised approaches, since it confirms that the 20 numeric features contain the information that are structuring the data into clusters, rather than the label itself providing a significant amount of this structure. If we saw the score drop when the label was removed, this would suggest that the label was necessary for dividing the data into classes and the features themselves did not contain sufficient predictive information to do so.

Similarly, we see that the score remains high for three different groups of features. These three groups are the groups we see with different coloured boarders in the boxplots. This means that even the features with very little correlation with the labels provide structure to the data. This could suggest that there are other ways that the data could be structured when using unsupervised models.

Overall, we see from exploritory data analysis that this data contains high amounts of information usable for predictive modelling, and a high clustering tendency which is promising for unsupervised clustering. Multicolinarity has been observed among several features that may prevent models from performing well.

```{r}
data <- read_csv("data_clean.csv")

# will be used in unsupervised learning
data_labels <- data %>% select(label) %>% mutate(label = as.factor(label))
data_features <- data %>% select(-label)
```

## Supervised Learning Methods

### logistic regression - including class weighting and L2 regularization, and feature selection.

A simple model, logistic regression is quick to implement and will reveal more about the data, in particular how different features contribute to predictions.

There are variations such as weighting and regularization which will be implemented - the success of lack of success of these techniques will reveal characteristics about our data that will be valuable to inform the selection and implementation of other models

### Random forest, including feature selection and tuning of mtry.

Random Forest was chosen due to its resilience. One of the more robust option, it is a good choice for handling the data without extra processing. Several characteristics of the data have been identified that may cause othe models such as logistic regression and SVM to struggle: - Correlated of features -Features that appeared to have low linear correlations with the label values (from heatmap in eda) but still contributed to the predictive ability of the model. This suggests there might be some non-linear relationships between features and the target variable - Features that aren't perfectly normally distributed, such as the bimodal peak in X7

Random Forest is a robust algorithm with few underlying assumptions that will handle these considerations well. Random Forest resists overfitting because of the sampling approach, it handles non linearity well, and it is naturally suited to multinomial classifications problems, like the one we have with four possible values for label. I also think that tree based models may peform well at distinguishing class A from class D, which was the biggest challenge that held back our logistic regression modelling. This is because it can prioritize at an early node in the tree a feature such as X11, which is one of the few that had high importance for descerning between class A and D, and then refine the selection in further nodes.

### SVM, with feature selection and tuning of kernel selection, gamma and cost parameters.

SVM is a powerful and popular algorith. SVM has options for different kernals that can be tuned, and this is a promising approach to solving the challenges of seperating class A from class D that is evedent from the data analysis and the results of logistic regression. It might be the case that A and D aren't linearly seperable, but a non-linear kernal will have success.

## Unsupervised Learning Methods

### Agglomerative hierarchical clustering, including tuning of linking metric.

Agglomerative heirarchical clustering was chosen because it is interesting to explore a model where the number of clusters is not specified and let the natural structure of the data reveal itself.

Biological data is often naturally heirarchical, for example animals can be classified by deviding them into first kingdoms, then families of species, and finally species and sub-species.

Although we don't know the exact meaning of each feature in our data, we know that it is biological in origin, perhaps gene expressions or environmental factors. This means that there might be a heirarchy of classes in our data.

Using this method without specifying that there are four values for label might reveal that some of the labels have a strong tendency to form sub classes, or that there is little structure in the data to justify asserting there are four classes. These would both be interesting finds.

It is also a model that handles the feature correlation well, which allows us to keep in all the collumns that correlate like X7, X8, and X9.

### Gaussian mixture model based clustering, including selecting a model, regularization using shrinkage parameter, and dimensionality reduction using factor analysis.

So far while working with this data set, we have struggled to seperate class D, and by plotting the results of some of the methods in specific dimensions, we have been able to show that class D significantly overlaps the other classes. We have also seen from the two dimensional PCA scatterplot that this is general overlap between all the classes in the the first two principle componants of the data.

Lots of clustering methods struggle with seperating overlapping clusters, so for the final method I wanted to choose one that might perform better with this challenge in mind. I have chosen to try a gaussian mixture model - a model-based clustering technique that assumes that all the data is distributed according to the combination of different normal distributions. I think it might have a good chance of performing well on our dataset because it is probabalistic, calculating the probability that a data point is in each cluster. This can help it perform better than other methods like K-means when there aren't clear boundaries between the clusters such as we see with class D.

Another advantage it has is that it has some flexibility in the geometry of the clusters it produces, unlike k-means which tends to produce spherical clusters. This is important because we have seen that in some dimensions our classes produce fairly elipsoidal clusters. It also operates on very different fundemental principles to our other unsupervised method - agglomerative heirarchical clustering - so it will be good to compare the two. If model-based clustering performs much better then it could suggest that the classes of our data are not heirarchical in nature.

# Results

## Exploratory Data Analysis Findings

most features are normally distributed except for X8 which is highly skewed. Features originally had different scales. X7, X8, X9 columns are correlated and correlate highly with the labels. X17, X18, X19 and X20 are highly correlated together and have very low correlation with the labels.

The PCA showed the four labels had some clustered structure, but also some significant overlap. The hopkins statistic showed there was a moderate clustering tendency, but that the label column when included made the clustering tendency extremely high. This is an initial suggestion that supervised learning would be more effective than unsupervised learning

## Supervised Learning Results

### Logistic Regression

```{r simple-logistic-regression-confusion-matrix}
#| cache: true

data <- read_csv("data_clean.csv")
data$label <- as.factor(data$label) # factor is like an enum, means A,B,C,D are read as categories not strings
set.seed(2626) # set the seed at the top so the entire doc is reproducable
# create test and train data splits we can use for the rest of the modelling
training.samples <- data$label %>% 
  createDataPartition(p = 0.8, list = FALSE) # create 80:20 split for train:test
train_data  <- data[training.samples, ]
test_data <- data[-training.samples, ]

simple_logreg_model <- multinom(
    label ~ ., # use all cols
    data = train_data,
    trace = FALSE
    )
simple_logreg_predictions <- predict(simple_logreg_model, test_data, type = "class")
simple_logreg_conf_matrix = confusionMatrix(simple_logreg_predictions, test_data$label)
simple_logreg_conf_df <- as.data.frame(simple_logreg_conf_matrix$table)

simple_logreg_conf_df <- simple_logreg_conf_df %>% pivot_wider(names_from = Reference, values_from = Freq)

simp_logreg_confm_table <- simple_logreg_conf_df %>%
  kable(
    caption = "Simple Logistic Regression Confusion Matrix", 
    longtable = FALSE,
    digits = 7, 
    align = "c",
    booktabs = TRUE,
    na="-"
  ) %>% 
  kable_styling(
    latex_options = c("striped", "hold_position", "scale_down"),
    font_size = 9
  ) %>%
  add_header_above(
    c(" " = 1, "Reference" = 4)
  ) #%>% 
  # pack_rows(
  #   "Prediction",
  #   1,
  #   4
  # )

full_width_table(simp_logreg_confm_table)


simp_logreg_overall_stats <- data.frame(
  Statistic = names(simple_logreg_conf_matrix$overall),
  Value = simple_logreg_conf_matrix$overall
)


simp_logreg_overall_stats_table <- simp_logreg_overall_stats %>%
  kable(
    caption = "Simple Logistic Regression Overall Statistics",
    col.names = c("Statistic", "Value"),
    longtable = FALSE,
    digits = 4,
    align = "lc",
    booktabs = TRUE
  ) %>%
  kable_styling(
    latex_options = c("striped", "hold_position"),
    font_size = 9
  )

full_width_table(simp_logreg_overall_stats_table)



simp_logreg_byClass_stats <- as.data.frame(simple_logreg_conf_matrix$byClass)
simp_logreg_byClass_stats <- t(simp_logreg_byClass_stats) # I want the class names to be columns not rows
simp_logreg_byClass_stats <- as.data.frame(simp_logreg_byClass_stats)


simp_logreg_byClass_stats_table <- simp_logreg_byClass_stats %>%
  rownames_to_column("Statistic") %>% # turn the statistics into a column
  kable(
    caption = "Simple Logistic Regression Statistics by Class",
    digits = 4,
    align = "lccccc",
    longtable = FALSE,
    booktabs = TRUE
  ) %>%
  kable_styling(
    latex_options = c("striped", "hold_position", "scale_down"),
    font_size = 9
  )

full_width_table(simp_logreg_byClass_stats_table)
```

```{r simple-logistic-regression-roc-curve, fig.cap="**Simple Logistic Regression ROC Plot:** *ROC plot for Class D vs Not Class D*"}
#| cache: true

simple_logreg_probabilities <- predict(simple_logreg_model, test_data, type = "probs") # using type=probs gives a probability of the label value. This lets us know when the model is making confident or unconfident predictions
is_class_d <- ifelse(test_data$label =="D", 1, 0)
class_d_probs <- as.data.frame(simple_logreg_probabilities)$D

# these produce a 1 and 0 for true and false for the actual value of class D for each row in data, and then the probability of each row being class d according to the model

roc <- roc(is_class_d, class_d_probs)
auc_value <- auc(roc)

plot(roc, main = paste("ROC Curve: Class D vs. All (AUC =", round(auc_value, 3), ")"),
     col = "blue", lwd = 2, 
     xlab = "False Positive Rate (1 - Specificity)", 
     ylab = "True Positive Rate (Sensitivity)")


abline(a = 0, b = 1, lty = 2, col = "gray")
```

```{r logistic-regression-feature-selection}
#| cache: true

train_data_logreg_fs1 <- train_data %>% select(X1, X2, X3, X4, X5, X6, X7, X8, X9, X10, X11, X12, X13, X14, X15, X16, X18, label)
test_data_logreg_fs1 <- test_data %>% select(X1, X2, X3, X4, X5, X6, X7, X8, X9, X10, X11, X12, X13, X14, X15, X16, X18, label)
logreg_featselection_1_model <- multinom(
    label ~ ., # use all cols
    data = train_data_logreg_fs1,
    trace = FALSE
  )
logreg_featselection_1_predictions <- predict(logreg_featselection_1_model, test_data_logreg_fs1, type = "class") # using type-class gives a prediction of the 
logreg_featselection_1_conf_matrix = confusionMatrix(logreg_featselection_1_predictions, test_data_logreg_fs1$label)

logreg_featselection_1_conf_df <- as.data.frame(logreg_featselection_1_conf_matrix$table)

logreg_featselection_1_conf_df <- logreg_featselection_1_conf_df %>% pivot_wider(names_from = Reference, values_from = Freq)

logreg_featselection_1_confm_table <- logreg_featselection_1_conf_df %>%
  kable(
    caption = "Simple Logistic Regression with selected features (X17,X19,X20 removed) Confusion Matrix", 
    digits = 7, 
    align = "c",
    booktabs = TRUE,
    longtable = FALSE,
    na="-"
  ) %>% 
  kable_styling(
    latex_options = c("striped", "hold_position", "scale_down"),
    font_size = 9
  ) %>%
  add_header_above(
    c(" " = 1, "Reference" = 4)
  ) #%>% 
  # pack_rows(
  #   "Prediction",
  #   1,
  #   4
  # )

full_width_table(logreg_featselection_1_confm_table)


logreg_featselection_1_overall_stats <- data.frame(
  Statistic = names(logreg_featselection_1_conf_matrix$overall),
  Value = logreg_featselection_1_conf_matrix$overall
)


logreg_featselection_1_overall_stats_table <- logreg_featselection_1_overall_stats %>%
  kable(
    caption = "Simple Logistic Regression with selected features (X17,X19,X20 removed) Overall Statistics",
    col.names = c("Statistic", "Value"),
    digits = 4,
    align = "lc",
    longtable = FALSE,
    booktabs = TRUE
  ) %>%
  kable_styling(
    latex_options = c("striped", "hold_position"),
    font_size = 9
  )

full_width_table(logreg_featselection_1_overall_stats_table)



logreg_featselection_1_byClass_stats <- as.data.frame(logreg_featselection_1_conf_matrix$byClass)
logreg_featselection_1_byClass_stats <- t(logreg_featselection_1_byClass_stats) # I want the class names to be columns not rows
logreg_featselection_1_byClass_stats <- as.data.frame(logreg_featselection_1_byClass_stats)


logreg_featselection_1_byClass_stats_table <- logreg_featselection_1_byClass_stats %>%
  rownames_to_column("Statistic") %>% # turn the statistics into a column
  kable(
    caption = "Simple Logistic Regression with selected features (X17,X19,X20 removed) Statistics by Class",
    longtable = FALSE,
    digits = 4,
    align = "lccccc",
    booktabs = TRUE
  ) %>%
  kable_styling(
    latex_options = c("striped", "hold_position", "scale_down"),
    font_size = 9
  )

full_width_table(logreg_featselection_1_byClass_stats_table)






train_data_logreg_fs2 <- train_data %>% select(X1, X2, X3, X4, X5, X6, X7, X10, X11, X12, X13, X14, X15, X16, X17, X18, X19, X20, label)
test_data_logreg_fs2 <- test_data %>% select(X1, X2, X3, X4, X5, X6, X7, X10, X11, X12, X13, X14, X15, X16, X17, X18, X19, X20, label)
logreg_featselection_2_model <- multinom(
    label ~ ., # use all cols
    data = train_data_logreg_fs2,
    trace = FALSE
  )
logreg_featselection_2_predictions <- predict(logreg_featselection_2_model, test_data_logreg_fs2, type = "class") # using type-class gives a prediction of the 
logreg_featselection_2_conf_matrix = confusionMatrix(logreg_featselection_2_predictions, test_data_logreg_fs2$label)

logreg_featselection_2_conf_df <- as.data.frame(logreg_featselection_2_conf_matrix$table)

logreg_featselection_2_conf_df <- logreg_featselection_2_conf_df %>% pivot_wider(names_from = Reference, values_from = Freq)

logreg_featselection_2_confm_table <- logreg_featselection_2_conf_df %>%
  kable(
    caption = "Simple Logistic Regression with selected features (X8,X9 removed) Confusion Matrix", 
    digits = 7, 
    longtable = FALSE,
    align = "c",
    booktabs = TRUE,
    na="-"
  ) %>% 
  kable_styling(
    latex_options = c("striped", "hold_position", "scale_down"),
    font_size = 9
  ) %>%
  add_header_above(
    c(" " = 1, "Reference" = 4)
  ) #%>% 
  # pack_rows(
  #   "Prediction",
  #   1,
  #   4
  # )

full_width_table(logreg_featselection_2_confm_table)


logreg_featselection_2_overall_stats <- data.frame(
  Statistic = names(logreg_featselection_2_conf_matrix$overall),
  Value = logreg_featselection_2_conf_matrix$overall
)


logreg_featselection_2_overall_stats_table <- logreg_featselection_2_overall_stats %>%
  kable(
    caption = "Simple Logistic Regression with selected features (X8,X9 removed) Overall Statistics",
    col.names = c("Statistic", "Value"),
    longtable = FALSE,
    digits = 4,
    align = "lc",
    booktabs = TRUE
  ) %>%
  kable_styling(
    latex_options = c("striped", "hold_position"),
    font_size = 9
  )

full_width_table(logreg_featselection_2_overall_stats_table)



logreg_featselection_2_byClass_stats <- as.data.frame(logreg_featselection_2_conf_matrix$byClass)
logreg_featselection_2_byClass_stats <- t(logreg_featselection_2_byClass_stats) # I want the class names to be columns not rows
logreg_featselection_2_byClass_stats <- as.data.frame(logreg_featselection_2_byClass_stats)


logreg_featselection_2_byClass_stats_table <- logreg_featselection_2_byClass_stats %>%
  rownames_to_column("Statistic") %>% # turn the statistics into a column
  kable(
    caption = "Simple Logistic Regression with selected features (X8,X9 removed) Statistics by Class",
    digits = 4,
    longtable = FALSE,
    align = "lccccc",
    booktabs = TRUE
  ) %>%
  kable_styling(
    latex_options = c("striped", "hold_position", "scale_down"),
    font_size = 9
  )

full_width_table(logreg_featselection_2_byClass_stats_table)
```

```{r weighted-logistic-regression}
#| cache: true

class_weights <- train_data %>%
  count(label) %>%
  mutate(weight = 1/n, # n just always gets the value for number of observations
         weight = weight/sum(weight) * n()) # regularize using the mean

weights_vector <- train_data %>%
  left_join(class_weights, by = "label") %>%
  pull(weight)  # gets just the weight column as a vector

weighted_model <- multinom(label ~ ., data = train_data, 
                          weights = weights_vector,
                          trace = FALSE)

weighted_pred <- predict(weighted_model, test_data)
weighted_logreg_conf_matrix <- confusionMatrix(weighted_pred, test_data$label)

weighted_logreg_conf_df <- as.data.frame(weighted_logreg_conf_matrix$table)

weighted_logreg_conf_df <- weighted_logreg_conf_df %>% pivot_wider(names_from = Reference, values_from = Freq)

weighted_logreg_confm_table <- weighted_logreg_conf_df %>%
  kable(
    caption = "Weighted Logistic Regression Confusion Matrix", 
    digits = 7, 
    align = "c",
    longtable = FALSE,
    booktabs = TRUE,
    na="-"
  ) %>% 
  kable_styling(
    latex_options = c("striped", "hold_position", "scale_down"),
    font_size = 9
  ) %>%
  add_header_above(
    c(" " = 1, "Reference" = 4)
  ) #%>% 
  # pack_rows(
  #   "Prediction",
  #   1,
  #   4
  # )

full_width_table(weighted_logreg_confm_table)


weighted_logreg_overall_stats <- data.frame(
  Statistic = names(weighted_logreg_conf_matrix$overall),
  Value = simple_logreg_conf_matrix$overall
)


weighted_logreg_overall_stats_table <- weighted_logreg_overall_stats %>%
  kable(
    caption = "Weighted Logistic Regression Overall Statistics",
    col.names = c("Statistic", "Value"),
    digits = 4,
    longtable = FALSE,
    align = "lc",
    booktabs = TRUE
  ) %>%
  kable_styling(
    latex_options = c("striped", "hold_position"),
    font_size = 9
  )

full_width_table(weighted_logreg_overall_stats_table)



weighted_logreg_byClass_stats <- as.data.frame(weighted_logreg_conf_matrix$byClass)
weighted_logreg_byClass_stats <- t(weighted_logreg_byClass_stats) # I want the class names to be columns not rows
weighted_logreg_byClass_stats <- as.data.frame(weighted_logreg_byClass_stats)


weighted_logreg_byClass_stats_table <- weighted_logreg_byClass_stats %>%
  rownames_to_column("Statistic") %>% # turn the statistics into a column
  kable(
    caption = "Weighted Logistic Regression Statistics by Class",
    digits = 4,
    longtable = FALSE,
    align = "lccccc",
    booktabs = TRUE
  ) %>%
  kable_styling(
    latex_options = c("striped", "hold_position", "scale_down"),
    font_size = 9
  )

full_width_table(weighted_logreg_byClass_stats_table)

```

```{r regularized-logistic-regression-summary-table}
#| cache: true

decay_values <- c(0.001, 0.01, 0.1, 0.5, 1, 2, 10) # these are the regularization parameters we are going to try

# creating a df to store the results of all models - will turn this into the report table
results_df <- data.frame(
  Decay = as.character(decay_values),
  Accuracy = numeric(length(decay_values)),
  Kappa = numeric(length(decay_values)),
  F1_Class_A = numeric(length(decay_values)),
  F1_Class_B = numeric(length(decay_values)),
  F1_Class_C = numeric(length(decay_values)),
  F1_Class_D = numeric(length(decay_values))
)

for (i in 1:length(decay_values)) {
  decay <- decay_values[i]
  model <- multinom(label ~ ., data = train_data, 
                   decay = decay, 
                   trace = FALSE)
  predictions <- predict(model, test_data)
  conf_matrix <- confusionMatrix(predictions, test_data$label)
  
  results_df$Accuracy[i] <- conf_matrix$overall["Accuracy"]
  results_df$Kappa[i] <- conf_matrix$overall["Kappa"]
  
  class_metrics <- conf_matrix$byClass
  class_names <- rownames(class_metrics)
  
  for (j in 1:length(class_names)) {
    class_name <- class_names[j]
    class_letter <- substr(class_name, nchar(class_name), nchar(class_name))
    col_name <- paste0("F1_Class_", class_letter)
    results_df[i, col_name] <- class_metrics[j, "F1"]
  }
  
  if (i == 1 || conf_matrix$overall["Accuracy"] > best_accuracy) {
    best_accuracy <- conf_matrix$overall["Accuracy"]
    best_decay <- decay
    best_conf_matrix <- conf_matrix
    best_predictions <- predictions
  }
}


decay_summary_table <- results_df %>%
  kable(
    caption = "Logistic Regression Performance with Different Regularization Parameters",
    col.names = c("Decay", "Accuracy", "Kappa", 
                  "F1 Score (A)", "F1 Score (B)", "F1 Score (C)", "F1 Score (D)"),
    digits = 4,
    longtable = FALSE,
    align = "lcccccc",
    booktabs = TRUE
  ) %>%
  kable_styling(
    latex_options = c("striped", "hold_position", "scale_down"),
    font_size = 9
  )
full_width_table(decay_summary_table)
```

```{r regularized-logistic-regression-viz, fig.cap="**Regularized Logistic Regression**"}
#| cache: true

ggplot(results_df %>% mutate(Decay = as.numeric(Decay)) %>% pivot_longer(cols = c(Accuracy, F1_Class_A, F1_Class_B, F1_Class_C, F1_Class_D),
                                  names_to = "Metric", values_to = "Value")) +
  geom_line(aes(x = Decay, y = Value, color = Metric)) +
  geom_point(aes(x = Decay, y = Value, color = Metric)) +
  scale_x_log10() +  # Log scale for better visualization of decay range
  labs(title = "Effect of Regularization on Logistic Regression Performance",
       x = "Decay Parameter (log scale)",
       y = "Performance Metric Value") +
  theme_minimal()
```

```{r regularized-logistic-regression-conf-matrix}
#| cache: true

best_reg_conf_df <- as.data.frame(best_conf_matrix$table)
best_reg_conf_df <- best_reg_conf_df %>% pivot_wider(names_from = Reference, values_from = Freq)
best_reg_conf_table <- best_reg_conf_df %>%
  kable(
    caption = paste("Confusion Matrix for Best Regularized Logistic Regression Model (Decay =", best_decay, ")"),
    digits = 0,
    longtable = FALSE,
    align = "c",
    booktabs = TRUE
  ) %>% 
  kable_styling(
    latex_options = c("striped", "hold_position"),
    font_size = 9
  ) %>%
  add_header_above(
    c(" " = 1, "Reference" = 4)
  )
full_width_table(best_reg_conf_table)

```

logistic regression was not great. weighting did nothing, as expected. regularization didn't really do anything. feature selection did not improve the model. We saw that the model particularly underperformed at classifying class D correctly.

### Random Forest

```{r random-forest-base}
#| cache: true

rf_model <- randomForest(
  label ~ ., 
  data = train_data,
  ntree = 500,           # number of trees we will use to start - can tune this later
  mtry = floor(sqrt(20)), # a typical default is sqrt(num of features) features per tree in the forest. can tune this later
  importance = TRUE       
)
rf_predictions <- predict(rf_model, test_data)
rf_conf_matrix <- confusionMatrix(rf_predictions, test_data$label)

rf_conf_df <- as.data.frame(rf_conf_matrix$table)
rf_conf_df <- rf_conf_df %>% pivot_wider(names_from = Reference, values_from = Freq)

rf_confm_table <- rf_conf_df %>%
  kable(
    caption = "Basic Random Forest Confusion Matrix", 
    digits = 7, 
    longtable = FALSE,
    align = "c",
    booktabs = TRUE,
    na="-"
  ) %>% 
  kable_styling(
    latex_options = c("striped", "hold_position", "scale_down"),
    font_size = 9
  ) %>%
  add_header_above(
    c(" " = 1, "Reference" = 4)
  ) #%>% 
  # pack_rows(
  #   "Prediction",
  #   1,
  #   4
  # )

full_width_table(rf_confm_table)


rf_overall_stats <- data.frame(
  Statistic = names(rf_conf_matrix$overall),
  Value = rf_conf_matrix$overall
)


rf_overall_stats_table <- rf_overall_stats %>%
  kable(
    caption = "Basic Random Forest Overall Statistics",
    col.names = c("Statistic", "Value"),
    digits = 4,
    longtable = FALSE,
    align = "lc",
    booktabs = TRUE
  ) %>%
  kable_styling(
    latex_options = c("striped", "hold_position"),
    font_size = 9
  )

full_width_table(rf_overall_stats_table)



rf_byClass_stats <- as.data.frame(rf_conf_matrix$byClass)
rf_byClass_stats <- t(rf_byClass_stats) # I want the class names to be columns not rows
rf_byClass_stats <- as.data.frame(rf_byClass_stats)


rf_byClass_stats_table <- rf_byClass_stats %>%
  rownames_to_column("Statistic") %>% # turn the statistics into a column
  kable(
    caption = "Basic Random Forest Statistics by Class",
    longtable = FALSE,
    digits = 4,
    align = "lccccc",
    booktabs = TRUE
  ) %>%
  kable_styling(
    latex_options = c("striped", "hold_position", "scale_down"),
    font_size = 9
  )

full_width_table(rf_byClass_stats_table)
```

```{r random-forest-base-feature-importance-table}
#| cache: true

importance_values <- as.data.frame(importance(rf_model) ) %>% 
  arrange(desc(MeanDecreaseAccuracy))
feature_importance_table <- importance_values %>%
  kable(
    caption = "Basic Random Forest Feature Importance", 
    digits = 3, 
    align = "c",
    booktabs = TRUE,
    longtable = FALSE,
    na="-"
  ) %>% 
  kable_styling(
    latex_options = c("striped", "hold_position", "scale_down"),
    font_size = 9
  )

full_width_table(feature_importance_table)
```

```{r random-forest-remove-unimportant-features}
#| cache: true

selected_train_data_1 <- train_data %>%
  select(-X17, -X19, -X20, -X15, -X6)
selected_test_data_1 <- test_data %>%
  select(-X17, -X19, -X20, -X15, -X6)
rf_feat_selection_1_model <- randomForest(
  label ~ ., 
  data = selected_train_data_1,
  ntree = 500,           # number of trees we will use to start - can tune this later
  mtry = floor(sqrt(20)), # a typical default is sqrt(num of features) features per tree in the forest. can tune this later
  importance = TRUE       
)

rf_fs_1_predictions <- predict(rf_feat_selection_1_model, selected_test_data_1)

rf_fs_1_conf_matrix <- confusionMatrix(rf_fs_1_predictions, selected_test_data_1$label)


rf_fs_1_conf_df <- as.data.frame(rf_fs_1_conf_matrix$table)
rf_fs_1_conf_df <- rf_fs_1_conf_df %>% pivot_wider(names_from = Reference, values_from = Freq)

rf_fs_1_confm_table <- rf_fs_1_conf_df %>%
  kable(
    caption = "Random Forest (5 least important features removed) Confusion Matrix", 
    digits = 7, 
    align = "c",
    booktabs = TRUE,
    longtable = FALSE,
    na="-"
  ) %>% 
  kable_styling(
    latex_options = c("striped", "hold_position", "scale_down"),
    font_size = 9
  ) %>%
  add_header_above(
    c(" " = 1, "Reference" = 4)
  ) #%>% 
  # pack_rows(
  #   "Prediction",
  #   1,
  #   4
  # )

full_width_table(rf_fs_1_confm_table)


rf_fs_1_overall_stats <- data.frame(
  Statistic = names(rf_fs_1_conf_matrix$overall),
  Value = rf_fs_1_conf_matrix$overall
)


rf_fs_1_overall_stats_table <- rf_fs_1_overall_stats %>%
  kable(
    caption = "Random Forest (5 least important features removed) Overall Statistics",
    col.names = c("Statistic", "Value"),
    digits = 4,
    align = "lc",
    longtable = FALSE,
    booktabs = TRUE
  ) %>%
  kable_styling(
    latex_options = c("striped", "hold_position"),
    font_size = 9
  )

full_width_table(rf_fs_1_overall_stats_table)



rf_fs_1_byClass_stats <- as.data.frame(rf_fs_1_conf_matrix$byClass)
rf_fs_1_byClass_stats <- t(rf_fs_1_byClass_stats) # I want the class names to be columns not rows
rf_fs_1_byClass_stats <- as.data.frame(rf_fs_1_byClass_stats)


rf_fs_1_byClass_stats_table <- rf_fs_1_byClass_stats %>%
  rownames_to_column("Statistic") %>% # turn the statistics into a column
  kable(
    caption = "Random Forest (5 least important features removed) Statistics by Class",
    digits = 4,
    longtable = FALSE,
    align = "lccccc",
    booktabs = TRUE
  ) %>%
  kable_styling(
    latex_options = c("striped", "hold_position", "scale_down"),
    font_size = 9
  )

full_width_table(rf_fs_1_byClass_stats_table)
```

```{r random-forest-remove-unimportant-features-feature-importance}
#| cache: true

rf_fs_1_importance_values <- as.data.frame(importance(rf_feat_selection_1_model) ) %>% 
  arrange(desc(MeanDecreaseAccuracy))
rf_fs_1_feature_importance_table <- rf_fs_1_importance_values %>%
  kable(
    caption = "Random Forest (5 least important features removed) Feature Importance", 
    digits = 3, 
    align = "c",
    booktabs = TRUE,
    longtable = FALSE,
    na="-"
  ) %>% 
  kable_styling(
    latex_options = c("striped", "hold_position", "scale_down"),
    font_size = 9
  )

full_width_table(rf_fs_1_feature_importance_table)
```

```{r random-forest-remove-important-features}
#| cache: true

selected_train_data_2 <- train_data %>%
  select(-X8)
selected_test_data_2 <- test_data %>%
  select(-X8)
rf_feat_selection_2_model <- randomForest(
  label ~ ., 
  data = selected_train_data_2,
  ntree = 500,           # number of trees we will use to start - can tune this later
  mtry = floor(sqrt(20)), # a typical default is sqrt(num of features) features per tree in the forest. can tune this later
  importance = TRUE       
)

rf_fs_2_predictions <- predict(rf_feat_selection_2_model, selected_test_data_2)

rf_fs_2_conf_matrix <- confusionMatrix(rf_fs_2_predictions, selected_test_data_2$label)


rf_fs_2_conf_df <- as.data.frame(rf_fs_2_conf_matrix$table)
rf_fs_2_conf_df <- rf_fs_2_conf_df %>% pivot_wider(names_from = Reference, values_from = Freq)

rf_fs_2_confm_table <- rf_fs_2_conf_df %>%
  kable(
    caption = "Random Forest (5 least important features removed) Confusion Matrix", 
    digits = 7, 
    longtable = FALSE,
    align = "c",
    booktabs = TRUE,
    na="-"
  ) %>% 
  kable_styling(
    latex_options = c("striped", "hold_position", "scale_down"),
    font_size = 9
  ) %>%
  add_header_above(
    c(" " = 1, "Reference" = 4)
  ) #%>% 
  # pack_rows(
  #   "Prediction",
  #   1,
  #   4
  # )

full_width_table(rf_fs_2_confm_table)


rf_fs_2_overall_stats <- data.frame(
  Statistic = names(rf_fs_2_conf_matrix$overall),
  Value = rf_fs_2_conf_matrix$overall
)


rf_fs_2_overall_stats_table <- rf_fs_2_overall_stats %>%
  kable(
    caption = "Random Forest (5 least important features removed) Overall Statistics",
    col.names = c("Statistic", "Value"),
    digits = 4,
    longtable = FALSE,
    align = "lc",
    booktabs = TRUE
  ) %>%
  kable_styling(
    latex_options = c("striped", "hold_position"),
    font_size = 9
  )

full_width_table(rf_fs_2_overall_stats_table)



rf_fs_2_byClass_stats <- as.data.frame(rf_fs_2_conf_matrix$byClass)
rf_fs_2_byClass_stats <- t(rf_fs_2_byClass_stats) # I want the class names to be columns not rows
rf_fs_2_byClass_stats <- as.data.frame(rf_fs_2_byClass_stats)


rf_fs_2_byClass_stats_table <- rf_fs_2_byClass_stats %>%
  rownames_to_column("Statistic") %>% # turn the statistics into a column
  kable(
    caption = "Random Forest (5 least important features removed) Statistics by Class",
    digits = 4,
    longtable = FALSE,
    align = "lccccc",
    booktabs = TRUE
  ) %>%
  kable_styling(
    latex_options = c("striped", "hold_position", "scale_down"),
    font_size = 9
  )

full_width_table(rf_fs_2_byClass_stats_table)
```

```{r random-forest-remove-important-features-feature-importance}
#| cache: true

rf_fs_2_importance_values <- as.data.frame(importance(rf_feat_selection_2_model) ) %>% 
  arrange(desc(MeanDecreaseAccuracy))
rf_fs_2_feature_importance_table <- rf_fs_2_importance_values %>%
  kable(
    caption = "Random Forest (5 least important features removed) Feature Importance", 
    digits = 3, 
    longtable = FALSE,
    align = "c",
    booktabs = TRUE,
    na="-"
  ) %>% 
  kable_styling(
    latex_options = c("striped", "hold_position", "scale_down"),
    font_size = 9
  )

full_width_table(rf_fs_2_feature_importance_table)
```

```{r tuning-random-forest, fig.cap="**Optimal mtry for Random Forest**"}
#| cache: true

train_data_no_label <- train_data %>% select(- label)

tuneRF_result <- tuneRF(
  x = train_data_no_label,
  y = train_data$label,
  ntreeTry = 500,
  mtryStart = 4,         # = sqrt(num of features) as before
  stepFactor = 1.5,      # this func trys a step above and below the starting value of mtry by multiplying by this number. 1.5 is the smallest step we can be sure will round to a number bigger/smaller that our stat
  improve = 0.01,        # model only has to imrove slightly for tuning to continue
  trace = FALSE, # turn off trace for report          
  plot = TRUE            
)


optimal_mtry <- tuneRF_result[which.min(tuneRF_result[, "OOBError"]), "mtry"]

```

```{r random-forest-tuned-results}
#| cache: true

rf_tuned_model <- randomForest(
  label ~ ., 
  data = train_data,
  ntree = 500,           # number of trees we will use to start - can tune this later
  mtry = 9, # optimal value from tuning with RFTune
  importance = TRUE       
)


rf_tuned_predictions <- predict(rf_tuned_model, test_data)
rf_tuned_conf_matrix <- confusionMatrix(rf_tuned_predictions, test_data$label)

rf_tuned_conf_df <- as.data.frame(rf_tuned_conf_matrix$table)
rf_tuned_conf_df <- rf_tuned_conf_df %>% pivot_wider(names_from = Reference, values_from = Freq)

rf_tuned_confm_table <- rf_tuned_conf_df %>%
  kable(
    caption = "Tuned Random Forest Confusion Matrix", 
    digits = 7, 
    longtable = FALSE,
    align = "c",
    booktabs = TRUE,
    na="-"
  ) %>% 
  kable_styling(
    latex_options = c("striped", "hold_position", "scale_down"),
    font_size = 9
  ) %>%
  add_header_above(
    c(" " = 1, "Reference" = 4)
  ) #%>% 
  # pack_rows(
  #   "Prediction",
  #   1,
  #   4
  # )

full_width_table(rf_tuned_confm_table)


rf_tuned_overall_stats <- data.frame(
  Statistic = names(rf_conf_matrix$overall),
  Value = rf_conf_matrix$overall
)


rf_tuned_overall_stats_table <- rf_tuned_overall_stats %>%
  kable(
    caption = "Tuned Random Forest Overall Statistics",
    longtable = FALSE,
    col.names = c("Statistic", "Value"),
    digits = 4,
    align = "lc",
    booktabs = TRUE
  ) %>%
  kable_styling(
    latex_options = c("striped", "hold_position"),
    font_size = 9
  )

full_width_table(rf_tuned_overall_stats_table)



rf_tuned_byClass_stats <- as.data.frame(rf_tuned_conf_matrix$byClass)
rf_tuned_byClass_stats <- t(rf_tuned_byClass_stats) # I want the class names to be columns not rows
rf_tuned_byClass_stats <- as.data.frame(rf_tuned_byClass_stats)


rf_tuned_byClass_stats_table <- rf_tuned_byClass_stats %>%
  rownames_to_column("Statistic") %>% # turn the statistics into a column
  kable(
    caption = "Tuned Random Forest Statistics by Class",
    digits = 4,
    longtable = FALSE,
    align = "lccccc",
    booktabs = TRUE
  ) %>%
  kable_styling(
    latex_options = c("striped", "hold_position", "scale_down"),
    font_size = 9
  )

full_width_table(rf_tuned_byClass_stats_table)

```

The random forest performed well without any configuration. feature selection was not effective. still struggled at seperating class D. mtry was tuned.

### SVM

```{r svm-kernel-selection}
#| cache: true

svm_linear <- svm(
  label ~ .,
  data = train_data,
  kernel = "linear",
  cost = 1, # will tune later - low value has softer margin on decision boundary. increasing will reduce misclassification, but increase risk of overfitting
  scale = FALSE       # data is already scaled during preprocessing
)

svm_poly_3 <- svm(
  label ~ .,
  data = train_data,
  kernel = "polynomial",
  degree = 3,         # polynomial degree, lets just try a few
  cost = 1,
  scale = FALSE
)

svm_poly_5 <- svm(
  label ~ .,
  data = train_data,
  kernel = "polynomial",
  degree = 5,         # polynomial degree, lets just try a few
  cost = 1,
  scale = FALSE
)

svm_poly_7 <- svm(
  label ~ .,
  data = train_data,
  kernel = "polynomial",
  degree = 7,         # polynomial degree, lets just try a few
  cost = 1,
  scale = FALSE
)


svm_radial <- svm(
  label ~ .,
  data = train_data,
  kernel = "radial",
  gamma = 0.05,       # tuneable parameter. idk what it does though
  cost = 1,
  scale = FALSE
)



svm_sigmoid <- svm(
  label ~ .,
  data = train_data,
  kernel = "sigmoid",
  gamma = 0.05,     # controls how steep/flat the sigmoid curve is. lower value is flatter and gives softer decision boundary
  coef0 = 0.5,    # Controls the y intercept shift of the sigmoid function - if 0 then the sigmoid goes throught he origin
  cost = 1,
  scale = FALSE   
)

models <- list(svm_linear, svm_poly_3, svm_poly_5, svm_poly_7, svm_radial, svm_sigmoid)

model_names <- c("Linear", "Polynomial - Order 3", "Polynomial - Order 5", "Polynomial - Order 7", "Radial", "Sigmoid")


results_df <- data.frame(
  Kernel = character(length(models)),
  Accuracy = numeric(length(models)),
  Kappa = numeric(length(models)),
  F1_Class_A = numeric(length(models)),
  F1_Class_B = numeric(length(models)),
  F1_Class_C = numeric(length(models)),
  F1_Class_D = numeric(length(models))
)

# im making sure the levels of the class labels are the same between test and train to rule this out as a reason the confusion matrix cant be calculated

lvls <- levels(train_data$label)
test_data$label <- factor(test_data$label, levels = lvls)

for (i in 1:length(models)) {
  model <- models[[i]]
  model_name <- model_names[[i]]
  
  predictions <- predict(model, test_data)
  conf_matrix <- confusionMatrix(predictions, test_data$label)
  
  results_df$Kernel[[i]] <- model_names[[i]]
  results_df$Accuracy[[i]] <- conf_matrix$overall["Accuracy"]
  results_df$Kappa[[i]] <- conf_matrix$overall["Kappa"]
  
  class_metrics <- conf_matrix$byClass
  class_names <- rownames(class_metrics)
  
  for (j in 1:length(class_names)) {
    class_name <- class_names[j]
    class_letter <- substr(class_name, nchar(class_name), nchar(class_name))
    col_name <- paste0("F1_Class_", class_letter)
    results_df[i, col_name] <- class_metrics[j, "F1"]
  }
  
  if (i == 1 || conf_matrix$overall["Accuracy"] > best_accuracy) {
    best_accuracy <- conf_matrix$overall["Accuracy"]
    best_kernel <- model_name
    best_conf_matrix <- conf_matrix
    best_predictions <- predictions
  }
}


svm_kernel_summary_table <- results_df %>%
  kable(
    caption = "SVM Performance with Different Kernels",
    col.names = c("Kernel", "Accuracy", "Kappa", 
                  "F1 Score (A)", "F1 Score (B)", "F1 Score (C)", "F1 Score (D)"),
    digits = 4,
    longtable = FALSE,
    align = "lcccccc",
    booktabs = TRUE
  ) %>%
  kable_styling(
    latex_options = c("striped", "hold_position", "scale_down"),
    font_size = 9
  )
full_width_table(svm_kernel_summary_table)
```

```{r svm-tune-hyperparameters, fig.cap="**Hyperparameter Affect on SVM Performance**"}
#| cache: true

tune_grid <- expand.grid(
  C = c(0.01, 0.1, 1, 10, 100), # values of the cost parameter, lower values have softer margin
  sigma = c(0.01, 0.05, 0.1, 0.5, 1) # values of the gamma parameter
)


svm_tune <- train(
  label ~ .,
  data = train_data,
  method = "svmRadial",
  preProcess = NULL, 
  tuneGrid = tune_grid,
  trControl = trainControl(method = "cv", number = 5) # cv is crossvalidation, 5 is the number of folds 
)
  

plot(svm_tune)

```


```{r svm-final-summary-tables}
#| cache: true

tune_grid <- expand.grid(
  C = c(0.164, 0.166, 0.168, 0.17, 0.172, 0.174, 0.176, 0.178, 0.18, 0.182, 0.185, 0.187, 0.19, 0.192, 0.195, 0.1975, 0.2, 0.202), # values of the cost parameter, lower values have softer margin
  sigma = c(0.038, 0.039, 0.04, 0.041, 0.044, 0.0445, 0.045, 0.0455, 0.046, 0.0465, 0.047, 0.048, 0.049, 0.05, 0.051, 0.052, 0.053, 0.054, 0.055) # values of the gamma parameter
)


svm_tune <- train(
  label ~ .,
  data = train_data,
  method = "svmRadial",
  preProcess = NULL,
  tuneGrid = tune_grid,
  trControl = trainControl(method = "repeatedcv", number = 10, repeats = 5) # cv is crossvalidation, 5 is the number of folds
  # with three repeats, the tuning is run 3 times and the average results are returned
)



# Extract optimal parameters
optimal_params <- svm_tune$bestTune
optimal_params



svm_final_model <- svm(
  label ~ .,
  data = train_data,
  kernel = "radial",
  gamma = optimal_params$sigma,
  cost = optimal_params$C,
  scale = FALSE
)

svm_final_predictions <- predict(svm_final_model, test_data)
svm_final_conf_matrix <- confusionMatrix(svm_final_predictions, test_data$label)


svm_final_conf_df <- as.data.frame(best_conf_matrix$table)
svm_final_conf_df <- svm_final_conf_df %>% pivot_wider(names_from = Reference, values_from = Freq)

svm_final_confm_table <- svm_final_conf_df %>%
  kable(
    caption = "SVM Tuned Model Confusion Matrix",
    digits = 7,
    longtable = FALSE,
    align = "c",
    booktabs = TRUE,
    na="-"
  ) %>%
  kable_styling(
    latex_options = c("striped", "hold_position", "scale_down"),
    font_size = 9
  ) %>%
  add_header_above(
    c(" " = 1, "Reference" = 4)
  ) #%>%
  # pack_rows(
  #   "Prediction",
  #   1,
  #   4
  # )

full_width_table(svm_final_confm_table)


svm_final_overall_stats <- data.frame(
  Statistic = names(rf_conf_matrix$overall),
  Value = rf_conf_matrix$overall
)


svm_final_overall_stats_table <- svm_final_overall_stats %>%
  kable(
    caption = "SVM Tuned Model Overall Statistics",
    col.names = c("Statistic", "Value"),
    digits = 4,
    longtable = FALSE,
    align = "lc",
    booktabs = TRUE
  ) %>%
  kable_styling(
    latex_options = c("striped", "hold_position"),
    font_size = 9
  )

full_width_table(svm_final_overall_stats_table)



svm_final_byClass_stats <- as.data.frame(svm_final_conf_matrix$byClass)
svm_final_byClass_stats <- t(svm_final_byClass_stats) # I want the class names to be columns not rows
svm_final_byClass_stats <- as.data.frame(svm_final_byClass_stats)


svm_final_byClass_stats_table <- svm_final_byClass_stats %>%
  rownames_to_column("Statistic") %>% # turn the statistics into a column
  kable(
    caption = "SVM Tuned Model Statistics by Class",
    longtable = FALSE,
    digits = 4,
    align = "lccccc",
    booktabs = TRUE
  ) %>%
  kable_styling(
    latex_options = c("striped", "hold_position", "scale_down"),
    font_size = 9
  )

full_width_table(svm_final_byClass_stats_table)


```

```{r plt-x7-vs-x8-misclassifications, fig.cap="**Misclassifications - X7 by X8**"}
#| cache: true

ggplot(test_data, aes(x = X7, y = X8, color = label, shape = label)) +
  geom_point(size = 3, alpha = 0.7) +
  labs(title = "Feature Space: X7 vs X8",
       x = "X7", y = "X8",
       color = "Actual Class",
       shape = "Actual Class") +
  theme_minimal()


svm_prediction_df <- data.frame(
  X7 = test_data$X7,
  X8 = test_data$X8,
  Actual = test_data$label,
  Predicted = svm_final_predictions,
  Correct = test_data$label == svm_final_predictions
)


ggplot(svm_prediction_df, aes(x = X7, y = X8, color = Predicted, shape = Actual)) +
  geom_point(size = 3, alpha = 0.7) +
  labs(title = "SVM Classification: X7 vs X8",
       x = "X7", y = "X8",
       color = "Predicted Class",
       shape = "Actual Class") +
  theme_minimal()


ggplot(svm_prediction_df, aes(x = X7, y = X8, color = Correct, shape = Actual)) +
  geom_point(size = 3, alpha = 0.7) +
  scale_color_manual(values = c("TRUE" = "green", "FALSE" = "red")) +
  labs(title = "SVM Misclassifications: X7 vs X8",
       x = "X7", y = "X8",
       color = "Correctly Classified",
       shape = "Actual Class") +
  theme_minimal()
```


svm was good but not as good as random forest. lots of tuning. feature selection was uneffective

## Unsupervised Learning Results

### Agglomerative Hierarchical Clustering



```{r silhouette-plot, fig.cap="**Silhouette Plot**"}
#| cache: true

fviz_nbclust(data_features, hcut, method = "silhouette", k.max = 10) +
  labs(title = "Optimal Number of Clusters - Silhouette Method")
```

```{r gap-stat-plot, fig.cap="**Gap Statistic Plot**"}
#| cache: true

fviz_nbclust(data_features, hcut, method = "gap_stat", k.max = 10, nboot = 50) +
  labs(title = "Optimal Number of Clusters - Gap Statistic")
```

```{r elbow-plot, fig.cap="**Elbow Graph**"}
#| cache: true

fviz_nbclust(data_features, hcut, method = "wss", k.max = 10) +
  labs(title = "Optimal Number of Clusters - Elbow Method")
```

ahc was good not great.

### Gaussian Mixed Model Clustering

```{r gmm-bic, results='hide', message=FALSE, warning=FALSE, fig.cap="**Baysian Information Criteria Plot for Choosing Optimal Model**"}
#| cache: true

# using this invisible function to stop printing to the console appearing in the pdf, courtesy of chatgpt
invisible({
  original_sink <- sink.number()
  sink(file = tempfile())
  

  #Fit Model Based clustering
  fitM <- Mclust(data_features, trace=FALSE)
  
  #Choose the model
  BIC <- mclustBIC(data_features, trace=FALSE)

  while (sink.number() > original_sink) sink()
})



plot(BIC)
```
VVE was found to be the best performing model. Promisingly, the plot peaked at four clusters. Since it is known that there actually are four categorical classes in the original data, this shows that the model is learning from the distributions of all four labels. Since we saw such overlap in classes and many misclassifications in class D previously, it would have been unsuprising to see three as the optimum number of classes,suggesting that data was better described by three categories than four. 

In heirarchical clustering, we didn't see clear confirmation of four groups from the graphs using either the elbow method, gap method, or silloette method.

The three letters in the model name describe the shape, orientation, and orientation of the clusters that the model predicts. In this case, VVE indicates that the model is predicting clusters that are elipsoids of equal orientation, but varying volume. This makes sense based on the PCA scatterplots where the data is shown as three roughly equally size elipses one above the other, with a fourth, larger elipses overlaying them, with the major axis of all four being roughly horizontal (along the first principle component)


```{r clustering-viz}
#| cache: true
fviz_mclust(fitM, "classification", geom = "point", pointsize = 1.5, palette = "jco")
```

This looks really similar to our original PCA plot, with class A B and C seperated and class D overlapping all three.


```{r gmm-conf-matrix-and-stats-tables}
#| cache: true

gmm_results <- data.frame(
  label = data_labels$label,
  prediction = fitM$classification
)


Mode <- function(x) {
  ux <- unique(x)
  ux[which.max(tabulate(match(x, ux)))]
}

# for each original label find which cluster contains it most frequently
label_to_cluster_map <- gmm_results %>%
  group_by(label) %>%
  summarise(modal_cluster = Mode(prediction), .groups = "drop") %>%
  arrange(label)  

# for each cluster find which original label appears most frequently
cluster_to_label_map <- gmm_results %>%
  group_by(prediction) %>%
  summarise(modal_label = Mode(label), .groups = "drop") %>%
  arrange(prediction)


# print("Label to Cluster mapping:")
# print(label_to_cluster_map)
# print("Cluster to Label mapping:")
# print(cluster_to_label_map)


map_cluster_to_best_label <- function(cluster) {
  map_value <- cluster_to_label_map$modal_label[cluster_to_label_map$prediction == cluster]
  if(length(map_value) == 0) return(NA)
  return(map_value)
}


gmm_results <- gmm_results %>%
  mutate(
    # map each cluster to its most common original label so the names are more meaningful in the table
    predicted_label = sapply(prediction, map_cluster_to_best_label),
    prediction = factor(prediction),
    predicted_label = factor(predicted_label, levels = levels(label))
  )


gmm_conf_matrix <- confusionMatrix(gmm_results$predicted_label, gmm_results$label)

gmm_conf_df <- as.data.frame(gmm_conf_matrix$table)

gmm_conf_df <- gmm_conf_df %>% pivot_wider(names_from = Reference, values_from = Freq)

gmm_confm_table <- gmm_conf_df %>%
  kable(
    caption = "Gaussian Mixture Model Confusion Matrix", 
    digits = 7, 
    align = "c",
    longtable = FALSE,
    booktabs = TRUE,
    na="-"
  ) %>% 
  kable_styling(
    latex_options = c("striped", "hold_position", "scale_down"),
    font_size = 9
  ) %>%
  add_header_above(
    c(" " = 1, "Reference" = 4)
  ) #%>% 
  # pack_rows(
  #   "Prediction",
  #   1,
  #   4
  # )

full_width_table(gmm_confm_table)


gmm_overall_stats <- data.frame(
  Statistic = names(gmm_conf_matrix$overall),
  Value = gmm_conf_matrix$overall
)


gmm_overall_stats_table <- gmm_overall_stats %>%
  kable(
    caption = "Gaussian Mixture Model Overall Statistics",
    col.names = c("Statistic", "Value"),
    longtable = FALSE,
    digits = 4,
    align = "lc",
    booktabs = TRUE
  ) %>%
  kable_styling(
    latex_options = c("striped", "hold_position"),
    font_size = 9
  )

full_width_table(gmm_overall_stats_table)



gmm_byClass_stats <- as.data.frame(gmm_conf_matrix$byClass)
gmm_byClass_stats <- t(gmm_byClass_stats) # I want the class names to be columns not rows
gmm_byClass_stats <- as.data.frame(gmm_byClass_stats)


gmm_byClass_stats_table <- gmm_byClass_stats %>%
  rownames_to_column("Statistic") %>% # turn the statistics into a column
  kable(
    caption = "Gaussian Mixture Model Statistics by Class",
    digits = 4,
    longtable = FALSE,
    align = "lccccc",
    booktabs = TRUE
  ) %>%
  kable_styling(
    latex_options = c("striped", "hold_position", "scale_down"),
    font_size = 9
  )

full_width_table(gmm_byClass_stats_table)

```

Gaussian mixed model clustering performed very well. It immediately had overall accuracy comparible with random forest, and performed notebly better than other models at seperating the fourth class successfully, with an F1 score of 0.8. 

Attempts were made to further tune the model through regularization:

```{r gmm-regularization-comparison}
#| cache: true


evaluate_gmm <- function(model, data_labels, model_name) {

  results <- data.frame(
    label = data_labels$label,
    prediction = model$classification
  )

  cluster_to_label_map <- results %>%
    group_by(prediction) %>%
    summarise(modal_label = names(which.max(table(label))), .groups = "drop")

  results$predicted_label <- sapply(results$prediction, function(cluster) {
    map_val <- cluster_to_label_map$modal_label[cluster_to_label_map$prediction == cluster]
    if(length(map_val) == 0) return(NA) # we need this in case the model doesn't find four clusters
    return(map_val)
  })
  
  lvls = levels(factor(data_labels$label))
  results$predicted_label <- factor(results$predicted_label, levels = lvls)
  results$label <- factor(results$label, levels = lvls)

  ari <- adjustedRandIndex(model$classification, as.numeric(data_labels$label))
  
  conf_matrix <- confusionMatrix(results$predicted_label, results$label)
  
  accuracy <- conf_matrix$overall["Accuracy"]

  class_metrics <- conf_matrix$byClass
  f1_scores <- sapply(1:nrow(class_metrics), function(i) class_metrics[i, "F1"])
  names(f1_scores) <- c("F1_A", "F1_B", "F1_C", "F1_D")
  
  
  return(data.frame(
    Model = model_name,
    ARI = ari,
    Accuracy = accuracy,
    F1_A = f1_scores["F1_A"],
    F1_B = f1_scores["F1_B"],
    F1_C = f1_scores["F1_C"],
    F1_D = f1_scores["F1_D"]
  ))
  

}

# base model has no regularization
base_results <- evaluate_gmm(fitM, data_labels, "No regularization")


shrinkage_values <- c(0.001, 0.01, 0.05, 0.1, 0.5)
results_list <- list(base_results)


for (value in shrinkage_values) {
  model <- Mclust(data_features, G=4, 
                 prior=priorControl(functionName="defaultPrior", shrinkage=value))

  model_name <- paste0("Shrinkage = ", value)
  model_results <- evaluate_gmm(model, data_labels, model_name)

  results_list[[length(results_list) + 1]] <- model_results
}

gmm_comparison <- do.call(rbind, results_list)


gmm_comparison_table <- gmm_comparison %>%
  kable(
    caption = "Gaussian Mixture Model Performance with Different Regularization Settings",
    col.names = c("Model", "ARI", "Accuracy", 
                 "F1 Score (A)", "F1 Score (B)", "F1 Score (C)", "F1 Score (D)"),
    digits = 4,
    longtable = FALSE,
    align = "lccccccr",
    booktabs = TRUE,
    row.names = FALSE
  ) %>%
  kable_styling(
    latex_options = c("striped", "hold_position", "scale_down"),
    font_size = 9
  ) 


full_width_table(gmm_comparison_table)
```

The best performing model did not use regularization. 

The motivation for using regularization is to help the model perform better when using features that correlate together. Since regularization in fact hindered performance, another approach is to use factor analysis to reduce the dimensionality of the data. 
Factor analysis is suitable because it is suited to handling the groups of correlated features notable in the EDA results, and because the data is biological in origin. With biological data, there is often an underlying cause, like a gene expression, that can have many measurable implecations, like disease symptoms or physical characteristics. Because we know these mechanisms may exist in the source of our data, factor analysis is a good choice to reduce dimensions while preserving as much information as possible.
```{r fa-scree-plt, fig.cap="**Scree Plot:** *for Choosing Number of Factors for Dimensionality Reduction*"}
#| cache: true

invisible({
  # Redirect console output to a null connection
  output <- capture.output({
    # Run the factor analysis parallel analysis
    scree_plot <- fa.parallel(data_features, fa = "fa", fm = "ml")
  })
})
```

The parallel analysis results suggest that the optimum number of factors is 6. Using the maximum likelyhood method finds the number of factors where the value of the eigenvalue is above what would be expected by random chance.

The underlying values from the analysis show that the first two factors contribute the most, with a sharp drop after that. So a dimensionality reduction to two factors could be a reasonable option. We can also see that the seventh and eigth eigenvalues are not much smaller than the sixth, so swapping some of the smaller factors could also be a justifyable experiment.

```{r fa-loadings, fig.cap="**Factor Analysis Loadings:** *loadings values by feature*"}
#| cache: true

fa_result <- fa(data_features, 
                nfactors = 6,  
                rotate = "varimax",
                fm = "ml")

# print(fa_result$loadings, cutoff = 0.3)  # only interested in the larger values, its easier to read the results 


loadings_df <- as.data.frame(fa_result$loadings[,1:6])
loadings_df$Feature <- paste0("X", 1:20)
# loadings_df
loadings_long <- loadings_df %>%
  pivot_longer(
    cols = -Feature,
    names_to = "Factor", 
    values_to = "Loading" 
  )

# create heatmap to visualize
ggplot(loadings_long, aes(x = Factor, y = Feature, fill = Loading)) +
  geom_tile(color = "white") +
  scale_fill_gradient2(low = "darkblue", mid = "white", high = "darkred", 
                       midpoint = 0, limit = c(-1,1), space = "Lab",
                       name = "Loading") +
  theme_minimal() +
  labs(x = "Factor", y = "Original Feature") +
  # label the highest loading values
  geom_text(data = subset(loadings_long, abs(Loading) >= 0.3),
            aes(label = round(Loading, 2)), 
            color = "black", size = 3)
```

The loadings of features to factors shows the highly correlated features in the correlated groups are all heavily loaded to the same factor, which will succesfully decrease the multicolinearily present in the dataset.

```{r ggm-with-fa}
#| cache: true

factor_scores <- factor.scores(data_features, fa_result)$scores


fitM_fa <- Mclust(factor_scores)
gmm_fa_results <- data_labels %>% mutate(prediction = fitM_fa$classification) # %>% mutate(label = as.numeric(label))

label_to_cluster_map <- gmm_fa_results %>%
  group_by(label) %>%
  summarise(modal_cluster = Mode(prediction), .groups = "drop") %>%
  arrange(label)  

# for each cluster find which original label appears most frequently
cluster_to_label_map <- gmm_fa_results %>%
  group_by(prediction) %>%
  summarise(modal_label = Mode(label), .groups = "drop") %>%
  arrange(prediction)



map_cluster_to_best_label <- function(cluster) {
  map_value <- cluster_to_label_map$modal_label[cluster_to_label_map$prediction == cluster]
  if(length(map_value) == 0) return(NA)
  return(map_value)
}


gmm_fa_results <- gmm_fa_results %>%
  mutate(
    # map each cluster to its most common original label so the names are more meaningful in the table
    predicted_label = sapply(prediction, map_cluster_to_best_label),
    prediction = factor(prediction),
    predicted_label = factor(predicted_label, levels = levels(label))
  )


gmm_fa_conf_matrix <- confusionMatrix(gmm_fa_results$predicted_label, gmm_fa_results$label)

gmm_fa_conf_df <- as.data.frame(gmm_fa_conf_matrix$table)

gmm_fa_conf_df <- gmm_fa_conf_df %>% pivot_wider(names_from = Reference, values_from = Freq)

gmm_fa_confm_table <- gmm_fa_conf_df %>%
  kable(
    caption = "Gaussian Mixture Model Confusion Matrix", 
    digits = 7, 
    longtable = FALSE,
    align = "c",
    booktabs = TRUE,
    na="-"
  ) %>% 
  kable_styling(
    latex_options = c("striped", "hold_position", "scale_down"),
    font_size = 9
  ) %>%
  add_header_above(
    c(" " = 1, "Reference" = 4)
  ) #%>% 
  # pack_rows(
  #   "Prediction",
  #   1,
  #   4
  # )

full_width_table(gmm_fa_confm_table)


gmm_fa_overall_stats <- data.frame(
  Statistic = names(gmm_conf_matrix$overall),
  Value = gmm_conf_matrix$overall
)


gmm_fa_overall_stats_table <- gmm_fa_overall_stats %>%
  kable(
    caption = "Gaussian Mixture Model Overall Statistics",
    col.names = c("Statistic", "Value"),
    digits = 4,
    longtable = FALSE,
    align = "lc",
    booktabs = TRUE
  ) %>%
  kable_styling(
    latex_options = c("striped", "hold_position"),
    font_size = 9
  )

full_width_table(gmm_fa_overall_stats_table)



gmm_fa_byClass_stats <- as.data.frame(gmm_fa_conf_matrix$byClass)
gmm_fa_byClass_stats <- t(gmm_fa_byClass_stats) # I want the class names to be columns not rows
gmm_fa_byClass_stats <- as.data.frame(gmm_fa_byClass_stats)


gmm_fa_byClass_stats_table <- gmm_fa_byClass_stats %>%
  rownames_to_column("Statistic") %>% # turn the statistics into a column
  kable(
    caption = "Gaussian Mixture Model Statistics by Class",
    digits = 4,
    longtable = FALSE,
    align = "lccccc",
    booktabs = TRUE
  ) %>%
  kable_styling(
    latex_options = c("striped", "hold_position", "scale_down"),
    font_size = 9
  )

full_width_table(gmm_fa_byClass_stats_table)

```

Operating on the factors rather than the original features, overall accuracy is slightly higher and the balanced accuracy of cluster 3 (class D) is slightly lower. This is a trade off that requires more knowledge of the intended use of the model to make a choice between the two.

However, the fact that factor analysis leads to increased accuracy overall while decreasing the dimensionality of the data so far is an interesting finding.



#### Model Evaluation

```{r cluster-uncertainty, fig.cap="**Cluster Uncertainty Plot:** *Clusters shown with uncertain points by size*"}
#| cache: true

final_model <- fitM_fa

fviz_mclust(fitM, "uncertainty")
```

```{r uncertainty-in-classes}
#| cache: true

high_uncertainty <- which(fitM$uncertainty > 0.4)


# table(data_labels$label[high_uncertainty])

cluster_composition <- data.frame(
  cluster = fitM$classification,
  true_label = data_labels$label
) %>%
  group_by(cluster) %>%
  count(true_label) %>%
  mutate(percentage = n / sum(n) * 100) %>%
  arrange(cluster, desc(percentage))


uncertainty_table <- cluster_composition %>%
  kable(
    caption = "High Uncertainty points in each Class",
    digits = 4,
    longtable = FALSE,
    # align = "lccccc",
    booktabs = TRUE
  ) %>%
  kable_styling(
    latex_options = c("striped", "hold_position", "scale_down"),
    font_size = 9
  )

full_width_table(uncertainty_table)
```



# Discussion

The relative performance of different learning models can give some insight to the underlying structure of the data.
If random forest and agglomerative hierarchical clustering performed significantly better than the other models, this would be evidence that the data was heirarchical in nature. If logistic regression performed as well as any of the other models, it would indicate that the data was extremely structured and easy to model, with strong linear relationships between the predictive features and the labels. The results found that random forest performed best followed by the radial kernel SVM. Both of these algorithms are better at modelling with non-linear relationships. This suggests that the the relationship between the features and the label exhibits some non-linearity. 

There are also lessons learned from the challenge of seperating class D which arrose in every model that was implemented.
Class D was harder for every model to correctly predict. In the context of biological data there are a several explainations of why this would happen, including that D is a super-class in a hierarchy where the other three are sub classes. This appears unlikely due to the underperformance of agglomerative hierarchical clustering. 
Another explaination is that D could be a transition state between other classes. This is not supported by the visualization of the data where class D seems to form a differently shaped sigmoid to the other classes, and overlap all three. Its still possible that D represents an immature state that will later develop into one of the other three.
The uncertainy of the predictions of this class by an array of models with such different underlying principles suggests that the difficulty is not a limitation of any algorithm, and it is likely that the features of the data do not have the predictive power necessary for efficient seperation of class D. Better performing models could be developed if data was collected with additional variables.

At the outset of the project, it was reasonable to assume that the supervised learning would out-perform the unsupervised learning models. The gaussian mixture model achieving the highest performance metrics speaks to the high amounts of noise present within each cluster and the dataset as a whole, as performing well on these kinds of data is a hallmark of the gmm algorithm.

Factor analysis had some success as the gmm model was able to retain its high accuracy and increase the F1 score of class D with a considerable reduction in the dimensionality of the data, suggesting the existance of underlying biological mechanisms or environmental factors that lead to the observed features arrising. Although well performing statistical models were trained using the features and the factor analysis approach, better results mights be possible using the underlying factors themselves if it is possible to measure them.

The best models had good overall accuracy but caution is advised for implementing any model with this data due to the underperformance in class D - if false positives or false negatives in this class have serious implications, some models become immediately unusable. Examples were this would be the case include when the classes represent risk of side effects to a medication option.

# Conclusion

The results of this project demonstrate how advanced statistical techniques are relevent to fields of research using biological data, with the potential for powerful models evident.
The gaussian mixed model would be the one model that could be taken forward for use classifying new data collected or for generalizing to another data set because of its high overall accuracy but particularly because of its high balanced accuracy in class D compared to other models. 
The greatest limitation of the models trained in this project is the underperformance of classifying class D, which could be critical in some applications. The collection of data with more descriptive features could allow for the development of more powerful models using the same techniques, or the work in this project could be build upon to create more refined models that perform better for specific use cases.
Further research should focus on improving the results of the random forest and gaussian mixture model approaches. An algorithm such as A gradient boosting algorithm such as XGBoost would build upon the successful tree-based approach of random forest. With each tree in a gradient boosting approach able to learn from the one before and the high tunability of the model, a model that performs better at the shortcomings of models in this report is possible.
An exciting direction to take forward the success of gmm would be to experiment with semi-supervised learning. A sample of class labels supplied to a model based clustering algorithm to guide the initial assignment of cluster can lead to learning distributions that are more specific to each cluster, with better results.


# References






















